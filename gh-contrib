#!/usr/bin/env python3
"""
Fetch GitHub issues and PRs a user has interacted with in a given organization.

Uses the gh CLI for authentication.

Usage:
    ./gh-contrib --username <username> --org <org> [--days <days>]

Requirements:
    - gh CLI installed and authenticated (https://cli.github.com/)
"""

import argparse
import hashlib
import json
import os
import sqlite3
import subprocess
import sys
import tempfile
import threading
import time
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass
from datetime import datetime, timedelta, timezone
from pathlib import Path
from typing import Callable, Optional

# Optional import for trend mode
PLOTEXT_AVAILABLE = False
try:
    import plotext as plt
    PLOTEXT_AVAILABLE = True
except ImportError:
    pass

# Constants
SEARCH_LIMIT = 1000  # GitHub's maximum for search
MAX_WORKERS = 10
TITLE_TRUNCATE_LENGTH = 60
API_PAGE_SIZE = 100
SUBPROCESS_TIMEOUT = 30  # seconds
MAX_PAGINATION_PAGES = 10  # Safety limit for pagination
PASSIVE_INTERACTION_TYPES = frozenset({"review-requested", "assignee", "mentioned"})

# Retry configuration
MAX_RETRIES = 3
INITIAL_RETRY_DELAY = 1.0  # seconds

# Health metric extraction functions (used in hot paths)
HEALTH_METRIC_EXTRACTORS = {
    "open-issues": lambda m: float(m.open_issues),
    "open-prs": lambda m: float(m.open_prs),
    "new-issues": lambda m: float(m.new_issues),
    "new-prs": lambda m: float(m.new_prs),
    "days-since-release": lambda m: float(m.days_since_release) if m.days_since_release is not None else None,
    "avg-issue-response": lambda m: m.avg_issue_response_hours,
    "avg-pr-response": lambda m: m.avg_pr_response_hours,
    "avg-pr-cycle": lambda m: m.avg_pr_cycle_hours
}

# Health metric formatting functions (used in hot paths)
HEALTH_METRIC_FORMATTERS = {
    "open-issues": lambda m: str(m.open_issues),
    "open-prs": lambda m: str(m.open_prs),
    "new-issues": lambda m: str(m.new_issues),
    "new-prs": lambda m: str(m.new_prs),
    "days-since-release": lambda m: f"{m.days_since_release}d ({m.last_release_tag})" if m.days_since_release is not None and m.last_release_tag else (f"{m.days_since_release}d" if m.days_since_release is not None else "N/A"),
    "avg-issue-response": lambda m: f"{format_duration(m.avg_issue_response_hours)} (n={m.issue_response_sample_size})" if m.avg_issue_response_hours is not None else "N/A",
    "avg-pr-response": lambda m: f"{format_duration(m.avg_pr_response_hours)} (n={m.pr_response_sample_size})" if m.avg_pr_response_hours is not None else "N/A",
    "avg-pr-cycle": lambda m: f"{format_duration(m.avg_pr_cycle_hours)} (n={m.merged_pr_count})" if m.avg_pr_cycle_hours is not None else "N/A"
}
BACKOFF_FACTOR = 2.0
RETRYABLE_ERRORS = frozenset({"rate limit", "502", "503", "504", "timeout"})
VALID_METRICS = frozenset({
    "prs-authored", "prs-reviewed", "prs-commented",
    "issues-authored", "issues-commented", "total"
})
ALL_METRICS = ["prs-authored", "prs-reviewed", "prs-commented", "issues-authored", "issues-commented", "total"]

# Cache version - bump when cache format changes
CACHE_VERSION = 1

# Health stats constants
HEALTH_DB_VERSION = 1
HEALTH_METRICS = [
    "open-issues", "open-prs", "new-issues", "new-prs",
    "days-since-release", "avg-issue-response", "avg-pr-response", "avg-pr-cycle"
]
VALID_HEALTH_METRICS = frozenset(HEALTH_METRICS)

# Bot detection patterns
BOT_SUFFIXES = {"[bot]"}
KNOWN_BOTS = frozenset({
    "dependabot", "dependabot-preview", "renovate", "renovate-bot",
    "github-actions", "codecov", "codecov-commenter", "sonarcloud",
    "stale", "mergify", "semantic-release-bot", "greenkeeper"
})

HEALTH_SAMPLE_SIZE = 50  # Items to sample for response time


def parse_date_arg(value: str) -> datetime:
    """Parse a date argument in YYYY-MM-DD format for argparse."""
    try:
        return datetime.strptime(value, "%Y-%m-%d").replace(tzinfo=timezone.utc)
    except ValueError:
        raise argparse.ArgumentTypeError(f"Invalid date format: '{value}'. Use YYYY-MM-DD")


def get_last_complete_week() -> tuple[datetime, datetime]:
    """Return (start, end) of the last complete Mon-Sun week.

    Returns the Monday 00:00:00 and Sunday 23:59:59 of the most recent
    complete week (not including the current week).
    """
    today = datetime.now(timezone.utc)
    days_since_monday = today.weekday()
    # Last Monday (start of last complete week)
    week_start = today - timedelta(days=days_since_monday + 7)
    week_start = week_start.replace(hour=0, minute=0, second=0, microsecond=0)
    # Last Sunday (end of last complete week)
    week_end = week_start + timedelta(days=6, hours=23, minutes=59, seconds=59)
    return week_start, week_end


def validate_gh_cli() -> None:
    """Validate that gh CLI is installed and authenticated."""
    try:
        subprocess.run(
            ["gh", "auth", "status"],
            capture_output=True,
            check=True,
            timeout=5
        )
    except FileNotFoundError:
        print("Error: GitHub CLI (gh) not found.", file=sys.stderr)
        print("Install from https://cli.github.com/", file=sys.stderr)
        sys.exit(1)
    except subprocess.CalledProcessError:
        print("Error: GitHub CLI not authenticated.", file=sys.stderr)
        print("Run 'gh auth login' to authenticate.", file=sys.stderr)
        sys.exit(1)
    except subprocess.TimeoutExpired:
        print("Error: GitHub CLI auth check timed out.", file=sys.stderr)
        sys.exit(1)


def get_cache_dir() -> Path:
    """Return the cache directory, creating it if needed with secure permissions."""
    cache_dir = Path.home() / ".gh-contrib" / "cache"
    cache_dir.mkdir(parents=True, exist_ok=True, mode=0o700)
    return cache_dir


def get_cache_key(usernames: list[str], orgs: list[str], since_dt: datetime, end_dt: datetime) -> str:
    """Generate a cache key hash from query parameters."""
    sorted_usernames = ",".join(sorted(username.lower() for username in usernames))
    sorted_orgs = ",".join(sorted(org.lower() for org in orgs))
    since_date = since_dt.strftime("%Y-%m-%d")
    end_date = end_dt.strftime("%Y-%m-%d")

    key_parts = [sorted_usernames, sorted_orgs, since_date, end_date]
    combined_key = "|".join(key_parts)
    return hashlib.sha256(combined_key.encode()).hexdigest()[:16]


def load_from_cache(cache_key: str) -> Optional[dict]:
    """Load cached data if it exists and is valid."""
    cache_file = get_cache_dir() / f"{cache_key}.json"
    if not cache_file.exists():
        return None
    try:
        with open(cache_file) as f:
            data = json.load(f)
        if data.get("version") != CACHE_VERSION:
            return None
        return data
    except (json.JSONDecodeError, KeyError):
        return None


def save_to_cache(cache_key: str, params: dict, data: dict) -> None:
    """Save data to cache atomically to prevent corruption."""
    cache_dir = get_cache_dir()
    target_file = cache_dir / f"{cache_key}.json"
    cache_data = {
        "version": CACHE_VERSION,
        "created_at": datetime.now(timezone.utc).isoformat(),
        "params": params,
        "data": data,
    }

    # Write to temp file then atomically rename
    fd, temp_path = tempfile.mkstemp(dir=cache_dir, suffix=".json")
    try:
        with os.fdopen(fd, "w") as f:
            json.dump(cache_data, f, indent=2, default=str)
        os.replace(temp_path, target_file)
    except Exception:
        # Clean up temp file on error
        if os.path.exists(temp_path):
            os.unlink(temp_path)
        raise


def clear_cache() -> int:
    """Clear all cached data. Returns number of files removed."""
    cache_dir = get_cache_dir()
    count = 0
    for cache_file in cache_dir.glob("*.json"):
        cache_file.unlink()
        count += 1
    return count


@dataclass
class UserStats:
    """Statistics for a user's GitHub activity."""
    prs_authored: int = 0
    prs_reviewed: int = 0
    prs_commented: int = 0
    issues_authored: int = 0
    issues_commented: int = 0

    def total(self) -> int:
        """Return total activity count."""
        return sum([
            self.prs_authored,
            self.prs_reviewed,
            self.prs_commented,
            self.issues_authored,
            self.issues_commented
        ])

    def as_list(self) -> list[int]:
        """Return stats as a list for table formatting."""
        return [
            self.prs_authored,
            self.prs_reviewed,
            self.prs_commented,
            self.issues_authored,
            self.issues_commented
        ]


@dataclass
class FetchResult:
    """Result of fetching and processing GitHub activity."""
    active_items: list[dict]
    filtered_items: list[dict]
    user_stats: dict[str, UserStats]


@dataclass
class RepoHealthMetrics:
    """Health metrics snapshot for a repository for a specific week."""
    repo_full_name: str
    github_id: int
    node_id: str
    week_start: datetime
    week_end: datetime

    # Core counts
    open_issues: int
    open_prs: int
    new_issues: int
    new_prs: int

    # Release
    days_since_release: Optional[int]
    last_release_tag: Optional[str]

    # Response times with sample metadata
    avg_issue_response_hours: Optional[float]
    avg_pr_response_hours: Optional[float]
    issue_response_sample_size: int
    pr_response_sample_size: int

    # Cycle time
    avg_pr_cycle_hours: Optional[float]
    merged_pr_count: int


def format_duration(hours: Optional[float]) -> str:
    """Convert hours to human-readable duration format.

    Args:
        hours: Duration in hours (can be None)

    Returns:
        Human-readable duration string:
        - None -> "-"
        - < 24 hours -> "4.2h" or "4h 12m" format
        - >= 24 hours -> "2.5d" format
    """
    if hours is None:
        return "-"

    # Validate input for edge cases
    import math
    if not isinstance(hours, (int, float)) or math.isnan(hours) or math.isinf(hours) or hours < 0:
        return "-"

    # Handle days (>= 24 hours)
    if hours >= 24:
        days = hours / 24
        return f"{int(days)}d" if days == int(days) else f"{days:.1f}d"

    # Handle less than 1 hour (show in minutes)
    if hours < 1:
        return f"{round(hours * 60)}m"

    # Handle whole hours
    if hours == int(hours):
        return f"{int(hours)}h"

    # Handle hours with fractional part
    whole_hours = int(hours)
    remaining_minutes = round((hours - whole_hours) * 60)

    return f"{whole_hours}h {remaining_minutes}m" if remaining_minutes > 0 else f"{whole_hours}h"


def get_health_db_path() -> Path:
    """Return path to health stats database, creating directory if needed."""
    db_dir = Path.home() / ".gh-contrib"
    db_dir.mkdir(parents=True, exist_ok=True, mode=0o700)
    return db_dir / "health.db"


def init_health_db() -> sqlite3.Connection:
    """Initialize health database with schema and version tracking."""
    conn = sqlite3.connect(str(get_health_db_path()))
    conn.row_factory = sqlite3.Row
    conn.execute("PRAGMA foreign_keys = ON")
    conn.execute("PRAGMA busy_timeout = 5000")
    conn.execute("PRAGMA journal_mode = WAL")

    # Check if database is initialized
    cursor = conn.execute(
        "SELECT name FROM sqlite_master WHERE type='table' AND name='schema_info'"
    )

    if cursor.fetchone() is None:
        # Create new database
        conn.executescript("""
            CREATE TABLE schema_info (
                version INTEGER PRIMARY KEY,
                applied_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            );

            CREATE TABLE repos (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                github_id INTEGER NOT NULL UNIQUE,
                node_id TEXT NOT NULL,
                org TEXT NOT NULL,
                name TEXT NOT NULL,
                full_name TEXT NOT NULL,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            );

            CREATE TABLE health_snapshots (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                repo_id INTEGER NOT NULL,
                week_start DATE NOT NULL,
                week_end DATE NOT NULL,
                open_issues INTEGER NOT NULL,
                open_prs INTEGER NOT NULL,
                new_issues INTEGER NOT NULL,
                new_prs INTEGER NOT NULL,
                days_since_release INTEGER,
                last_release_tag TEXT,
                avg_issue_response_hours REAL,
                avg_pr_response_hours REAL,
                issue_response_sample_size INTEGER,
                pr_response_sample_size INTEGER,
                avg_pr_cycle_hours REAL,
                merged_pr_count INTEGER,
                collected_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (repo_id) REFERENCES repos(id) ON DELETE CASCADE,
                UNIQUE(repo_id, week_start)
            );

            CREATE INDEX idx_repos_github_id ON repos(github_id);
            CREATE INDEX idx_repos_org ON repos(org);
            CREATE INDEX idx_repos_full_name ON repos(full_name);
            CREATE INDEX idx_snapshots_repo_week ON health_snapshots(repo_id, week_start);
            CREATE INDEX idx_snapshots_week ON health_snapshots(week_start);
        """)
        conn.execute("INSERT INTO schema_info (version) VALUES (?)", (HEALTH_DB_VERSION,))
        conn.commit()
    else:
        # Handle future migrations
        cursor = conn.execute("SELECT MAX(version) FROM schema_info")
        current_version = cursor.fetchone()[0] or 0
        if current_version < HEALTH_DB_VERSION:
            conn.execute("INSERT INTO schema_info (version) VALUES (?)", (HEALTH_DB_VERSION,))
            conn.commit()

    return conn


def get_or_create_repo(
    conn: sqlite3.Connection,
    github_id: int,
    node_id: str,
    org: str,
    name: str,
    full_name: str
) -> int:
    """Get or create repository entry, handling renames via github_id."""
    cursor = conn.execute(
        "SELECT id, full_name FROM repos WHERE github_id = ?", (github_id,)
    )
    row = cursor.fetchone()

    if row:
        repo_id = row["id"]
        # Update if renamed
        if row["full_name"] != full_name:
            conn.execute(
                "UPDATE repos SET org = ?, name = ?, full_name = ?, updated_at = CURRENT_TIMESTAMP WHERE id = ?",
                (org, name, full_name, repo_id)
            )
            conn.commit()
        return repo_id

    # Create new repo, handling race condition where another process may have inserted
    try:
        cursor = conn.execute(
            "INSERT INTO repos (github_id, node_id, org, name, full_name) VALUES (?, ?, ?, ?, ?)",
            (github_id, node_id, org, name, full_name)
        )
        conn.commit()
        return cursor.lastrowid
    except sqlite3.IntegrityError:
        # Another process inserted the repo - re-select to get the existing repo_id
        cursor = conn.execute(
            "SELECT id FROM repos WHERE github_id = ?", (github_id,)
        )
        row = cursor.fetchone()
        return row["id"]


def save_health_snapshot(conn: sqlite3.Connection, metrics: RepoHealthMetrics) -> None:
    """Save or update health metrics snapshot for a repo and week."""
    # Parse org and name from full_name
    parts = metrics.repo_full_name.split("/", 1)
    org, name = parts[0], parts[1] if len(parts) > 1 else parts[0]

    # Get or create repo entry
    repo_id = get_or_create_repo(
        conn, metrics.github_id, metrics.node_id, org, name, metrics.repo_full_name
    )

    # Format dates for SQLite
    week_start_str = metrics.week_start.strftime("%Y-%m-%d")
    week_end_str = metrics.week_end.strftime("%Y-%m-%d")

    # UPSERT: insert or update on conflict
    conn.execute(
        """INSERT INTO health_snapshots (
               repo_id, week_start, week_end, open_issues, open_prs, new_issues, new_prs,
               days_since_release, last_release_tag, avg_issue_response_hours, avg_pr_response_hours,
               issue_response_sample_size, pr_response_sample_size, avg_pr_cycle_hours, merged_pr_count
           ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
           ON CONFLICT(repo_id, week_start) DO UPDATE SET
               week_end=excluded.week_end,
               open_issues=excluded.open_issues,
               open_prs=excluded.open_prs,
               new_issues=excluded.new_issues,
               new_prs=excluded.new_prs,
               days_since_release=excluded.days_since_release,
               last_release_tag=excluded.last_release_tag,
               avg_issue_response_hours=excluded.avg_issue_response_hours,
               avg_pr_response_hours=excluded.avg_pr_response_hours,
               issue_response_sample_size=excluded.issue_response_sample_size,
               pr_response_sample_size=excluded.pr_response_sample_size,
               avg_pr_cycle_hours=excluded.avg_pr_cycle_hours,
               merged_pr_count=excluded.merged_pr_count,
               collected_at=CURRENT_TIMESTAMP""",
        (
            repo_id, week_start_str, week_end_str, metrics.open_issues, metrics.open_prs,
            metrics.new_issues, metrics.new_prs, metrics.days_since_release, metrics.last_release_tag,
            metrics.avg_issue_response_hours, metrics.avg_pr_response_hours,
            metrics.issue_response_sample_size, metrics.pr_response_sample_size,
            metrics.avg_pr_cycle_hours, metrics.merged_pr_count
        )
    )
    conn.commit()


def load_health_snapshots(
    conn: sqlite3.Connection,
    github_ids: list[int],
    num_weeks: int
) -> dict[int, list[RepoHealthMetrics]]:
    """Load health snapshots for repos over recent weeks."""
    if not github_ids:
        return {}

    placeholders = ",".join("?" * len(github_ids))
    query = f"""
        SELECT r.github_id, r.node_id, r.full_name, s.week_start, s.week_end,
               s.open_issues, s.open_prs, s.new_issues, s.new_prs,
               s.days_since_release, s.last_release_tag,
               s.avg_issue_response_hours, s.avg_pr_response_hours,
               s.issue_response_sample_size, s.pr_response_sample_size,
               s.avg_pr_cycle_hours, s.merged_pr_count
        FROM health_snapshots s
        JOIN repos r ON s.repo_id = r.id
        WHERE r.github_id IN ({placeholders})
        ORDER BY r.github_id, s.week_start DESC
    """

    cursor = conn.execute(query, github_ids)
    results: dict[int, list[RepoHealthMetrics]] = {gid: [] for gid in github_ids}

    for row in cursor:
        github_id = row["github_id"]
        if len(results[github_id]) >= num_weeks:
            continue

        week_start = datetime.strptime(row["week_start"], "%Y-%m-%d").replace(tzinfo=timezone.utc)
        week_end = datetime.strptime(row["week_end"], "%Y-%m-%d").replace(
            hour=23, minute=59, second=59, tzinfo=timezone.utc
        )

        metrics = RepoHealthMetrics(
            repo_full_name=row["full_name"],
            github_id=github_id,
            node_id=row["node_id"],
            week_start=week_start,
            week_end=week_end,
            open_issues=row["open_issues"],
            open_prs=row["open_prs"],
            new_issues=row["new_issues"],
            new_prs=row["new_prs"],
            days_since_release=row["days_since_release"],
            last_release_tag=row["last_release_tag"],
            avg_issue_response_hours=row["avg_issue_response_hours"],
            avg_pr_response_hours=row["avg_pr_response_hours"],
            issue_response_sample_size=row["issue_response_sample_size"] or 0,
            pr_response_sample_size=row["pr_response_sample_size"] or 0,
            avg_pr_cycle_hours=row["avg_pr_cycle_hours"],
            merged_pr_count=row["merged_pr_count"] or 0
        )
        results[github_id].append(metrics)

    return results


def get_cached_weeks(conn: sqlite3.Connection, github_id: int) -> set[str]:
    """Get cached week_start dates for a repository."""
    cursor = conn.execute(
        "SELECT s.week_start FROM health_snapshots s JOIN repos r ON s.repo_id = r.id WHERE r.github_id = ?",
        (github_id,)
    )
    return {row["week_start"] for row in cursor}


def get_week_boundaries(num_weeks: int) -> list[tuple[datetime, datetime]]:
    """Calculate week boundaries for the last N complete weeks.

    Args:
        num_weeks: Number of weeks to generate boundaries for

    Returns:
        List of (week_start, week_end) tuples, ordered from oldest to newest
        Each week starts Monday 00:00:00 UTC and ends Sunday 23:59:59 UTC
    """
    today = datetime.now(timezone.utc)
    days_since_monday = today.weekday()

    # Find the Monday of the last complete week
    last_complete_monday = today - timedelta(days=days_since_monday + 7)
    last_complete_monday = last_complete_monday.replace(hour=0, minute=0, second=0, microsecond=0)

    boundaries = []
    for week_offset in range(num_weeks - 1, -1, -1):  # Go from oldest to newest
        week_start = last_complete_monday - timedelta(weeks=week_offset)
        week_end = week_start + timedelta(days=6, hours=23, minutes=59, seconds=59)
        boundaries.append((week_start, week_end))

    return boundaries


def list_org_repos(org: str) -> list[dict]:
    """Fetch all repositories in an organization.

    Args:
        org: Organization name

    Returns:
        List of repository dictionaries with id, node_id, name, full_name
    """
    repos = []
    page = 1

    while page <= MAX_PAGINATION_PAGES:
        endpoint = f"orgs/{org}/repos?per_page={API_PAGE_SIZE}&page={page}"
        result = gh_api_cached(endpoint)

        if not result or not isinstance(result, list):
            break

        # Extract minimal required fields with safe dictionary access
        required_fields = ["id", "node_id", "name", "full_name"]
        page_repos = [
            {field: repo.get(field) for field in required_fields}
            for repo in result
            if all(field in repo for field in required_fields)  # Only include repos with all required fields
        ]
        repos.extend(page_repos)

        # If we got less than a full page, we're done
        if len(result) < API_PAGE_SIZE:
            break

        page += 1

    return repos


def collect_repo_health_metrics(
    repo: dict,
    week_start: datetime,
    week_end: datetime,
    ignore_users: set[str],
    dry_run: bool = False
) -> Optional[RepoHealthMetrics]:
    """Collect all health metrics for one repository for one week.

    Args:
        repo: Repository dictionary with id, node_id, name, full_name
        week_start: Start of the week being measured
        week_end: End of the week being measured
        ignore_users: Set of usernames to ignore for response time calculations
        dry_run: If True, preview queries without executing

    Returns:
        RepoHealthMetrics object or None if collection failed
    """
    repo_full_name = repo["full_name"]

    try:
        # Fetch all health metrics using a single GraphQL query
        health_data = fetch_repo_health_graphql(repo_full_name, week_start, week_end, dry_run=dry_run)

        if not health_data or dry_run:
            return None

        # Parse the GraphQL response into structured metrics
        metrics = parse_graphql_health_response(
            health_data, repo_full_name, week_start, week_end, ignore_users
        )

        return metrics

    except Exception as e:
        print(f"Error collecting health metrics for {repo_full_name}: {e}", file=sys.stderr)
        return None


def collect_health_for_repos(
    repos: list[dict],
    num_weeks: int,
    ignore_users: set[str],
    refresh: bool = False,
    dry_run: bool = False
) -> dict[str, list[RepoHealthMetrics]]:
    """Collect health metrics for multiple repositories over multiple weeks.

    Uses producer-consumer pattern: threads collect metrics, main thread writes to SQLite.
    Only fetches weeks not already in cache (unless refresh=True).

    Args:
        repos: List of repository dictionaries with id, node_id, name, full_name
        num_weeks: Number of weeks to collect metrics for
        ignore_users: Set of usernames to ignore for response time calculations
        refresh: If True, collect data even for cached weeks
        dry_run: If True, preview queries without executing

    Returns:
        Dictionary mapping repo full_name to list of RepoHealthMetrics (newest first)
    """
    import queue
    from concurrent.futures import ThreadPoolExecutor, as_completed

    if dry_run:
        print("Dry run mode - would collect health metrics for:")
        week_boundaries = get_week_boundaries(num_weeks)
        for repo in repos[:5]:  # Show first 5 repos
            print(f"  {repo['full_name']}")
        print(f"  ... and {len(repos) - 5} more repos" if len(repos) > 5 else "")

        # Only run dry-run collection if we have repos
        if repos:
            for week_start, week_end in week_boundaries:
                collect_repo_health_metrics(repos[0], week_start, week_end, ignore_users, dry_run=True)
        else:
            print("  No repositories found to analyze")
        return {}

    # Initialize database
    conn = init_health_db()
    results_by_repo: dict[str, list[RepoHealthMetrics]] = {}

    try:
        # Get week boundaries
        week_boundaries = get_week_boundaries(num_weeks)

        # Build list of tasks (repo, week_start, week_end) that need collection
        tasks = []
        cached_count = 0
        total_tasks = 0

        for repo in repos:
            repo_github_id = repo["id"]
            cached_weeks = get_cached_weeks(conn, repo_github_id)

            for week_start, week_end in week_boundaries:
                total_tasks += 1
                week_start_str = week_start.strftime("%Y-%m-%d")

                if refresh or week_start_str not in cached_weeks:
                    tasks.append((repo, week_start, week_end))
                else:
                    cached_count += 1

        if cached_count > 0:
            print(f"Using cached data for {cached_count}/{total_tasks} repo-week combinations")

        if not tasks:
            print("All data already cached")
        else:
            print(f"Collecting fresh data for {len(tasks)} repo-week combinations...")

        # Display rate limit before collection
        if tasks:  # Only show if we're actually collecting data
            display_rate_limit_info("before collection", dry_run)

        # Queue for collecting results from worker threads
        results_queue = queue.Queue()

        def collect_worker(repo, week_start, week_end):
            """Worker function to collect metrics for one repo-week combination."""
            try:
                metrics = collect_repo_health_metrics(repo, week_start, week_end, ignore_users)
                if metrics:
                    results_queue.put(metrics)
            except Exception as e:
                print(f"Worker error for {repo['full_name']}: {e}", file=sys.stderr)

        # Execute collection tasks in parallel with immediate result processing
        written_count = 0
        if tasks:
            with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
                futures = []
                for repo, week_start, week_end in tasks:
                    future = executor.submit(collect_worker, repo, week_start, week_end)
                    futures.append(future)

                # Process results immediately as they become available
                for future in as_completed(futures):
                    try:
                        future.result()  # This will raise if the worker failed

                        # Check for new results in queue and write them immediately
                        while not results_queue.empty():
                            try:
                                metrics = results_queue.get_nowait()
                                save_health_snapshot(conn, metrics)
                                written_count += 1
                            except queue.Empty:
                                break
                            except Exception as e:
                                print(f"Database write error: {e}", file=sys.stderr)

                    except Exception as e:
                        print(f"Task execution error: {e}", file=sys.stderr)

        # Final cleanup: process any remaining results in queue
        while not results_queue.empty():
            try:
                metrics = results_queue.get_nowait()
                save_health_snapshot(conn, metrics)
                written_count += 1
            except queue.Empty:
                break
            except Exception as e:
                print(f"Database write error: {e}", file=sys.stderr)

        if written_count > 0:
            print(f"Saved {written_count} new health snapshots to database")

        # Display rate limit after collection
        if tasks:  # Only show if we actually collected data
            display_rate_limit_info("after collection", dry_run)

        # Load final results from database (combines fresh + cached data)
        github_ids = [repo["id"] for repo in repos]
        db_results = load_health_snapshots(conn, github_ids, num_weeks)

        # Convert to repo full_name keyed results
        for repo in repos:
            repo_github_id = repo["id"]
            repo_full_name = repo["full_name"]
            results_by_repo[repo_full_name] = db_results.get(repo_github_id, [])

        return results_by_repo

    finally:
        conn.close()


def gh_graphql(query: str, variables: Optional[dict] = None, dry_run: bool = False) -> Optional[dict]:
    """Execute a GraphQL query using gh CLI.

    Args:
        query: The GraphQL query string
        variables: Optional variables dictionary for the query
        dry_run: If True, print the query without executing it

    Returns:
        Parsed JSON response from GraphQL API, or None on error/dry_run
    """
    if dry_run:
        print("GraphQL Query:")
        print(query)
        if variables:
            print("\nVariables:")
            print(json.dumps(variables, indent=2, default=str))
        print()
        return None

    # Build command with query and variables
    cmd = ["gh", "api", "graphql", "-f", f"query={query}"]
    if variables:
        for key, value in variables.items():
            cmd.extend(["-F", f"{key}={value}"])

    delay = INITIAL_RETRY_DELAY

    for attempt in range(MAX_RETRIES + 1):
        try:
            result = subprocess.run(cmd, capture_output=True, text=True, check=True, timeout=SUBPROCESS_TIMEOUT)
            data = json.loads(result.stdout)

            # Check for GraphQL errors
            if "errors" in data:
                error_messages = [error.get("message", "Unknown error") for error in data["errors"]]
                print(f"GraphQL errors: {', '.join(error_messages)}", file=sys.stderr)
                return None

            return data.get("data")

        except subprocess.TimeoutExpired:
            if attempt < MAX_RETRIES:
                print(f"GraphQL query timed out, retrying in {delay}s...", file=sys.stderr)
                time.sleep(delay)
                delay *= BACKOFF_FACTOR
                continue
            print(f"Warning: GraphQL query timed out after {MAX_RETRIES} retries", file=sys.stderr)
            return None

        except subprocess.CalledProcessError as e:
            stderr_lower = e.stderr.lower() if e.stderr else ""
            is_retryable = any(err in stderr_lower for err in RETRYABLE_ERRORS)

            if is_retryable and attempt < MAX_RETRIES:
                msg = "rate limit hit" if "rate limit" in stderr_lower else "server error"
                print(f"GraphQL {msg}, retrying in {delay}s...", file=sys.stderr)
                time.sleep(delay)
                delay *= BACKOFF_FACTOR
                continue

            # Handle final error
            if "rate limit" in stderr_lower:
                print("Warning: GitHub GraphQL API rate limit exhausted", file=sys.stderr)
            elif "not found" in stderr_lower or "404" in stderr_lower:
                print("Warning: Repository not found or not accessible", file=sys.stderr)
            else:
                print(f"Error executing GraphQL query: {e.stderr}", file=sys.stderr)
            return None

        except json.JSONDecodeError:
            print("Warning: Invalid JSON response from GraphQL API", file=sys.stderr)
            return None

    return None


def get_github_rate_limit(dry_run: bool = False) -> tuple[int, str]:
    """Query GitHub GraphQL API rate limit status.

    Args:
        dry_run: If True, return mock data instead of making API call

    Returns:
        Tuple of (remaining_points, reset_time_str)
    """
    if dry_run:
        return (4500, "2026-01-12T17:00:00Z")

    query = """
    query {
        rateLimit {
            remaining
            resetAt
        }
    }
    """

    try:
        response = gh_graphql(query)
        # Handle both wrapped ({"data": {"rateLimit": {...}}}) and unwrapped responses
        if response:
            data = response.get("data", response) if isinstance(response, dict) else response
            if data and "rateLimit" in data:
                remaining = data["rateLimit"]["remaining"]
                reset_at = data["rateLimit"]["resetAt"]
                return (remaining, reset_at)
        return (0, "unknown")
    except Exception as e:
        print(f"Warning: Could not fetch rate limit info: {e}", file=sys.stderr)
        return (0, "unknown")


def display_rate_limit_info(label: str, dry_run: bool = False) -> None:
    """Display current GitHub API rate limit status.

    Args:
        label: Description label (e.g., "before collection", "after collection")
        dry_run: If True, show mock data
    """
    remaining, reset_at = get_github_rate_limit(dry_run)

    if remaining == 0 and reset_at == "unknown":
        print(f"Rate limit ({label}): Unable to fetch status")
        return

    # Parse reset time for better display (fallback to raw string on error)
    reset_str = reset_at
    try:
        from datetime import datetime
        # GitHub API returns ISO format: 2026-01-12T17:00:00Z
        reset_dt = datetime.fromisoformat(reset_at.replace('Z', '+00:00'))
        reset_str = reset_dt.strftime("%H:%M")
    except (ValueError, TypeError):
        pass  # Use raw reset_at value as fallback

    print(f"Rate limit ({label}): {remaining}/5000 remaining (resets at {reset_str})")

    # Warning if approaching limit
    if remaining < 1000:
        print(f"Warning: Rate limit is low ({remaining} remaining)", file=sys.stderr)


def fetch_repo_health_graphql(repo_full_name: str, since_dt: datetime, until_dt: datetime, dry_run: bool = False) -> Optional[dict]:
    """Fetch repository health metrics using a single GraphQL query.

    Args:
        repo_full_name: Repository in format "owner/repo"
        since_dt: Start datetime for new issues/PRs query
        until_dt: End datetime for new issues/PRs query
        dry_run: If True, preview query without executing

    Returns:
        Dictionary containing health metrics, or None on error/dry_run
    """
    try:
        owner, repo = repo_full_name.split("/", 1)
    except ValueError:
        print(f"Invalid repository format: {repo_full_name}. Expected 'owner/repo'", file=sys.stderr)
        return None

    # Format datetime for GraphQL (ISO 8601)
    since_iso = since_dt.isoformat()

    # Format datetime for search queries (without timezone for search compatibility)
    since_search = since_dt.strftime("%Y-%m-%d")
    until_search = until_dt.strftime("%Y-%m-%d")

    # Build search queries for new and merged PRs using bounded date ranges
    new_prs_query = f"repo:{repo_full_name} is:pr created:{since_search}..{until_search}"
    merged_prs_query = f"repo:{repo_full_name} is:pr is:merged merged:{since_search}..{until_search}"

    query = """
    query RepoHealth($owner: String!, $name: String!, $since: DateTime!, $queryNewPrs: String!, $queryMergedPrs: String!) {
      repository(owner: $owner, name: $name) {
        id
        databaseId

        # Open issues count
        openIssues: issues(states: OPEN) { totalCount }

        # Open PRs count
        openPRs: pullRequests(states: OPEN) { totalCount }

        # New issues in period
        newIssues: issues(filterBy: {since: $since}) { totalCount }

        # Sample of open issues for response time (most recent 50)
        issuesSample: issues(states: OPEN, first: 50, orderBy: {field: CREATED_AT, direction: DESC}) {
          nodes {
            createdAt
            author { login }
            comments(first: 10) {
              nodes {
                createdAt
                author { login }
              }
            }
          }
        }

        # Sample of open PRs for response time
        prsSample: pullRequests(states: OPEN, first: 50, orderBy: {field: CREATED_AT, direction: DESC}) {
          nodes {
            createdAt
            author { login }
            comments(first: 10) {
              nodes {
                createdAt
                author { login }
              }
            }
            reviews(first: 10) {
              nodes {
                createdAt
                author { login }
              }
            }
          }
        }

        # Merged PRs for cycle time calculation (recent 50)
        mergedPrsSample: pullRequests(states: MERGED, first: 50, orderBy: {field: UPDATED_AT, direction: DESC}) {
          nodes {
            createdAt
            mergedAt
            author { login }
          }
        }

        # Latest release
        latestRelease: releases(first: 1, orderBy: {field: CREATED_AT, direction: DESC}) {
          nodes {
            tagName
            publishedAt
            isDraft
            isPrerelease
          }
        }
      }

      # Search for new PRs in the period
      newPRs: search(query: $queryNewPrs, type: ISSUE, first: 0) { issueCount }

      # Search for merged PRs in the period
      mergedPRsCount: search(query: $queryMergedPrs, type: ISSUE, first: 0) { issueCount }
    }
    """

    variables = {
        "owner": owner,
        "name": repo,
        "since": since_iso,
        "queryNewPrs": new_prs_query,
        "queryMergedPrs": merged_prs_query
    }

    return gh_graphql(query, variables, dry_run=dry_run)


def is_bot_user(username: str, ignore_list_lower: set[str]) -> bool:
    """Check if a user appears to be a bot based on patterns and ignore lists.

    Args:
        username: GitHub username to check
        ignore_list_lower: Pre-computed lowercase set of usernames to treat as bots

    Returns:
        True if the user appears to be a bot
    """
    if not username:
        return True

    username_lower = username.lower()

    # Check custom ignore list, known bots, or bot suffix patterns
    return (
        username_lower in ignore_list_lower
        or username_lower in KNOWN_BOTS
        or any(username.endswith(suffix) for suffix in BOT_SUFFIXES)
    )


def calculate_response_time_hours(items: list[dict], ignore_users: set[str]) -> tuple[Optional[float], int]:
    """Calculate average response time in hours for a list of issues/PRs.

    Args:
        items: List of issues/PRs with author and comments/reviews data
        ignore_users: Set of usernames to ignore (bots, etc.)

    Returns:
        Tuple of (average_hours, sample_size) where average_hours is None if no qualifying responses
    """
    response_times = []

    # Pre-compute lowercase ignore users set for efficiency
    ignore_users_lower = {user.lower() for user in ignore_users}

    for item in items:
        author_login = item.get("author", {}).get("login", "").lower()
        created_at = parse_datetime(item["createdAt"])
        first_response_time = None

        # Check both comments and reviews using the same logic
        for response_type in ["comments", "reviews"]:
            for response in item.get(response_type, {}).get("nodes", []):
                response_author = response.get("author", {}).get("login", "")
                if not response_author:
                    continue

                response_author_lower = response_author.lower()

                # Skip if author or bot
                if (response_author_lower == author_login or
                    is_bot_user(response_author, ignore_users_lower)):
                    continue

                response_time = parse_datetime(response["createdAt"])
                if first_response_time is None or response_time < first_response_time:
                    first_response_time = response_time

        # Calculate response time if we found a qualifying response
        if first_response_time:
            response_time_hours = (first_response_time - created_at).total_seconds() / 3600
            response_times.append(response_time_hours)

    if not response_times:
        return None, len(items)

    return sum(response_times) / len(response_times), len(items)


def parse_graphql_health_response(data: dict, repo_full_name: str, week_start: datetime, week_end: datetime, ignore_users: set[str]) -> Optional[RepoHealthMetrics]:
    """Parse GraphQL response into RepoHealthMetrics.

    Args:
        data: GraphQL response data
        repo_full_name: Repository name in "owner/repo" format
        week_start: Start of the week being measured
        week_end: End of the week being measured
        ignore_users: Set of usernames to ignore for response time calculations

    Returns:
        RepoHealthMetrics object or None if parsing failed
    """
    repo = data.get("repository") if data else None
    if not repo:
        return None

    try:
        # Core counts (use .get() with nested defaults)
        open_issues = repo.get("openIssues", {}).get("totalCount", 0)
        open_prs = repo.get("openPRs", {}).get("totalCount", 0)
        new_issues = repo.get("newIssues", {}).get("totalCount", 0)

        # Get new and merged PR counts from search results
        new_prs = data.get("newPRs", {}).get("issueCount", 0)
        merged_pr_count = data.get("mergedPRsCount", {}).get("issueCount", 0)

        # Calculate response times
        avg_issue_response_hours, issue_response_sample_size = calculate_response_time_hours(
            repo.get("issuesSample", {}).get("nodes", []), ignore_users
        )
        avg_pr_response_hours, pr_response_sample_size = calculate_response_time_hours(
            repo.get("prsSample", {}).get("nodes", []), ignore_users
        )

        # Calculate PR cycle time from merged PRs sample
        merged_prs_sample = repo.get("mergedPrsSample", {}).get("nodes", [])
        cycle_times = []

        for pr in merged_prs_sample:
            created_at_str = pr.get("createdAt")
            merged_at_str = pr.get("mergedAt")

            if not (created_at_str and merged_at_str):
                continue

            try:
                created_at = parse_datetime(created_at_str)
                merged_at = parse_datetime(merged_at_str)

                # Only include PRs that were merged within our time period
                if not (week_start <= merged_at <= week_end):
                    continue

                cycle_time_hours = (merged_at - created_at).total_seconds() / 3600
                if cycle_time_hours >= 0:  # Sanity check
                    cycle_times.append(cycle_time_hours)
            except (ValueError, TypeError):
                continue  # Skip invalid timestamps

        avg_pr_cycle_hours = sum(cycle_times) / len(cycle_times) if cycle_times else None

        # Release information
        days_since_release = None
        last_release_tag = None
        releases = repo.get("latestRelease", {}).get("nodes", [])
        if releases:
            release = releases[0]
            published_at = release.get("publishedAt")
            if (published_at and not release.get("isDraft") and not release.get("isPrerelease")):
                release_time = parse_datetime(published_at)
                days_since_release = int((datetime.now(timezone.utc) - release_time).total_seconds() / 86400)
                last_release_tag = release.get("tagName")

        return RepoHealthMetrics(
            repo_full_name=repo_full_name,
            github_id=repo["databaseId"],
            node_id=repo["id"],
            week_start=week_start,
            week_end=week_end,
            open_issues=open_issues,
            open_prs=open_prs,
            new_issues=new_issues,
            new_prs=new_prs,
            days_since_release=days_since_release,
            last_release_tag=last_release_tag,
            avg_issue_response_hours=avg_issue_response_hours,
            avg_pr_response_hours=avg_pr_response_hours,
            issue_response_sample_size=issue_response_sample_size,
            pr_response_sample_size=pr_response_sample_size,
            avg_pr_cycle_hours=avg_pr_cycle_hours,
            merged_pr_count=merged_pr_count
        )

    except (KeyError, TypeError, ValueError) as e:
        print(f"Error parsing GraphQL response for {repo_full_name}: {e}", file=sys.stderr)
        return None


def is_pull_request(item: dict) -> bool:
    """Determine if an item is a pull request."""
    return (
        item.get("isPullRequest", False)
        or item["state"] == "MERGED"
        or "/pull/" in item["url"]
    )


def parse_interaction(interaction: str, default_username: Optional[str] = None) -> tuple[str, Optional[str]]:
    """Parse interaction string into base type and username.

    Args:
        interaction: String like "author (@username)" or "author"
        default_username: Username to use if not tagged

    Returns:
        Tuple of (base_interaction, username)
    """
    if " (@" in interaction:
        base = interaction.split(" (@")[0]
        username = interaction.split(" (@")[1].rstrip(")")
        return base, username
    return interaction, default_username


def group_by_repository(items: list[dict]) -> dict[str, list[dict]]:
    """Group items by repository full name (owner/repo)."""
    by_repo = defaultdict(list)
    for item in items:
        repo_name = item["repository"]["nameWithOwner"]
        by_repo[repo_name].append(item)
    return dict(by_repo)


def gh_search(args: list[str]) -> list[dict]:
    """Execute a gh search command and return results."""
    cmd = ["gh", "search", "issues"] + args + [
        "--include-prs",
        "--json", "url,title,state,repository,updatedAt,createdAt,author,assignees,isPullRequest,number",
        "--limit", str(SEARCH_LIMIT)
    ]

    try:
        result = subprocess.run(cmd, capture_output=True, text=True, check=True, timeout=SUBPROCESS_TIMEOUT)
        return json.loads(result.stdout)
    except subprocess.TimeoutExpired:
        print(f"Error: gh search timed out after {SUBPROCESS_TIMEOUT}s", file=sys.stderr)
        sys.exit(1)
    except subprocess.CalledProcessError as e:
        print(f"Error calling gh search: {e.stderr}", file=sys.stderr)
        sys.exit(1)
    except FileNotFoundError:
        print("Error: gh CLI not found. Install from https://cli.github.com/", file=sys.stderr)
        sys.exit(1)


def gh_api(endpoint: str) -> Optional[list | dict]:
    """Execute a GitHub API call using gh CLI with retry and exponential backoff.

    Retries on rate limits, server errors (502, 503, 504), and timeouts.
    Returns the parsed JSON response, or None on error.
    """
    cmd = ["gh", "api", endpoint]
    delay = INITIAL_RETRY_DELAY

    for attempt in range(MAX_RETRIES + 1):
        try:
            result = subprocess.run(cmd, capture_output=True, text=True, check=True, timeout=SUBPROCESS_TIMEOUT)
            return json.loads(result.stdout)
        except subprocess.TimeoutExpired:
            if attempt < MAX_RETRIES:
                print(f"API call timed out, retrying in {delay}s...", file=sys.stderr)
                time.sleep(delay)
                delay *= BACKOFF_FACTOR
                continue
            print(f"Warning: API call timed out after {MAX_RETRIES} retries: {endpoint}", file=sys.stderr)
            return None
        except subprocess.CalledProcessError as e:
            stderr_lower = e.stderr.lower() if e.stderr else ""
            is_retryable = any(err in stderr_lower for err in RETRYABLE_ERRORS)

            if is_retryable and attempt < MAX_RETRIES:
                if "rate limit" in stderr_lower:
                    print(f"Rate limit hit, waiting {delay}s before retry...", file=sys.stderr)
                else:
                    print(f"Server error, retrying in {delay}s...", file=sys.stderr)
                time.sleep(delay)
                delay *= BACKOFF_FACTOR
                continue

            if "rate limit" in stderr_lower:
                print(f"Warning: GitHub API rate limit exhausted: {endpoint}", file=sys.stderr)
            return None
        except json.JSONDecodeError:
            return None

    return None


class APICache:
    """Thread-safe cache for API responses using singleflight pattern.

    Prevents duplicate concurrent API calls and caches results for the
    duration of script execution.
    """

    def __init__(self):
        self._cache: dict[str, list | dict | None] = {}
        self._pending: dict[str, threading.Event] = {}
        self._lock = threading.Lock()

    def get(self, endpoint: str) -> tuple[bool, list | dict | None]:
        """Get cached value. Returns (found, value)."""
        with self._lock:
            if endpoint in self._cache:
                return True, self._cache[endpoint]
            return False, None

    def set(self, endpoint: str, value: list | dict | None) -> None:
        """Store value in cache."""
        with self._lock:
            self._cache[endpoint] = value

    def clear(self) -> None:
        """Clear all cached data."""
        with self._lock:
            self._cache.clear()
            self._pending.clear()

    def get_or_fetch(
        self,
        endpoint: str,
        fetch_fn: Callable[[str], list | dict | None]
    ) -> list | dict | None:
        """Get from cache or fetch using singleflight pattern.

        If multiple threads request the same endpoint concurrently, only one
        will perform the fetch while others wait for the result.
        """
        with self._lock:
            # Check if result is already cached
            if endpoint in self._cache:
                return self._cache[endpoint]
            # Check if another thread is already fetching this endpoint
            if endpoint in self._pending:
                event = self._pending[endpoint]
            else:
                # We're the first - create event and mark as pending
                event = threading.Event()
                self._pending[endpoint] = event
                event = None  # Signal that we should do the fetch

        if event is not None:
            # Wait for the other thread to complete the fetch
            event.wait()
            with self._lock:
                return self._cache.get(endpoint)

        # We're responsible for fetching
        try:
            result = fetch_fn(endpoint)
            with self._lock:
                self._cache[endpoint] = result
                pending_event = self._pending.pop(endpoint, None)
            if pending_event:
                pending_event.set()
            return result
        except Exception:
            # On error, still need to signal waiting threads and clean up
            with self._lock:
                pending_event = self._pending.pop(endpoint, None)
            if pending_event:
                pending_event.set()
            raise


# Module-level cache instance
_api_cache = APICache()


def gh_api_cached(endpoint: str) -> list | dict | None:
    """Execute a GitHub API call with caching to avoid redundant requests.

    This cache persists for the entire script execution, significantly reducing
    API calls when processing multiple users on the same items.
    """
    return _api_cache.get_or_fetch(endpoint, gh_api)


def parse_datetime(dt_str: str) -> datetime:
    """Parse ISO datetime string to datetime object."""
    # Handle both formats: with and without milliseconds
    dt_str = dt_str.replace("Z", "+00:00")
    return datetime.fromisoformat(dt_str)


def get_interaction_details(item: dict, username: str, since: datetime, until: datetime) -> tuple[list[str], Optional[datetime]]:
    """Determine how the user interacted with this issue/PR and when."""
    interactions = []
    latest_interaction: Optional[datetime] = None
    username_lower = username.lower()

    # Check if user is author (only count if created within date range)
    if item.get("author", {}).get("login", "").lower() == username_lower:
        if created_at := item.get("createdAt"):
            created_dt = parse_datetime(created_at)
            if since <= created_dt <= until:
                interactions.append("author")
                if latest_interaction is None or created_dt > latest_interaction:
                    latest_interaction = created_dt

    # Check if user is assignee (passive, no timestamp to check)
    assignees = item.get("assignees", [])
    if any(a.get("login", "").lower() == username_lower for a in assignees):
        interactions.append("assignee")

    return interactions, latest_interaction


def _fetch_all_pages(endpoint_base: str) -> list:
    """Fetch all pages of a paginated endpoint.

    Args:
        endpoint_base: The base endpoint URL (without query params)

    Returns:
        List of all items across all pages (up to MAX_PAGINATION_PAGES)
    """
    all_results = []

    for page in range(1, MAX_PAGINATION_PAGES + 1):
        endpoint = f"{endpoint_base}?per_page={API_PAGE_SIZE}&page={page}"
        result = gh_api_cached(endpoint)

        if not result or not isinstance(result, list):
            break

        all_results.extend(result)

        # If we got less than a full page, we're done
        if len(result) < API_PAGE_SIZE:
            break

    return all_results


def fetch_additional_interactions(item: dict, username: str, since: datetime, until: datetime) -> tuple[list[str], Optional[datetime]]:
    """Fetch additional interaction details from API (comments, reviews) with timestamps.

    Fetches all pages of comments and reviews to ensure complete data.
    """
    interactions = []
    latest_interaction: Optional[datetime] = None
    username_lower = username.lower()
    repo = item["repository"]["nameWithOwner"]
    number = item["number"]

    # Fetch ALL comments with pagination
    comments = _fetch_all_pages(f"repos/{repo}/issues/{number}/comments")
    if comments:
        user_comments = [
            c for c in comments
            if c.get("user", {}).get("login", "").lower() == username_lower
        ]
        # Filter by date and find latest
        for comment in user_comments:
            comment_dt = parse_datetime(comment["created_at"])
            if since <= comment_dt <= until:
                if "commenter" not in interactions:
                    interactions.append("commenter")
                if latest_interaction is None or comment_dt > latest_interaction:
                    latest_interaction = comment_dt

    # Fetch ALL PR reviews with pagination (if it's a PR)
    if is_pull_request(item):
        reviews = _fetch_all_pages(f"repos/{repo}/pulls/{number}/reviews")
        if reviews:
            user_reviews = [
                r for r in reviews
                if r.get("user", {}).get("login", "").lower() == username_lower
            ]
            # Filter by date and find latest
            for review in user_reviews:
                review_dt = parse_datetime(review["submitted_at"])
                if since <= review_dt <= until:
                    if "reviewer" not in interactions:
                        interactions.append("reviewer")
                    if latest_interaction is None or review_dt > latest_interaction:
                        latest_interaction = review_dt

    return interactions, latest_interaction


def process_item(item: dict, username: str, since: datetime, until: datetime) -> dict:
    """Process a single item to get all interaction details."""
    interactions, latest_basic_interaction = get_interaction_details(item, username, since, until)
    additional, latest_additional_interaction = fetch_additional_interactions(item, username, since, until)
    interactions.extend(additional)

    # Determine the latest interaction timestamp
    latest_interaction = max(
        filter(None, [latest_basic_interaction, latest_additional_interaction]),
        default=None
    )

    # If no specific interaction found, mark as "mentioned" (catch-all for involves)
    if not interactions:
        interactions.append("mentioned")

    item["_interactions"] = interactions
    item["_latest_interaction"] = latest_interaction
    return item


# Markdown special characters that need escaping
MARKDOWN_ESCAPE_CHARS = {
    "|": "\\|",   # Table cell separator
    "[": "\\[",   # Link start
    "]": "\\]",   # Link end
    "`": "\\`",   # Code block
    "*": "\\*",   # Bold/italic
    "_": "\\_",   # Bold/italic
    "#": "\\#",   # Headers
    "~": "\\~",   # Strikethrough
    "<": "&lt;",  # HTML entities
    ">": "&gt;",  # HTML entities
    "\n": " ",    # Newlines (replace, not escape)
    "\r": "",     # Carriage returns (remove)
}


def escape_markdown(text: str) -> str:
    """Escape characters that break markdown tables/links.

    Escapes: | [ ] ` * _ # ~ < >
    Replaces newlines with spaces and removes carriage returns.
    """
    result = text
    for char, replacement in MARKDOWN_ESCAPE_CHARS.items():
        result = result.replace(char, replacement)
    return result


def format_table_row(item: dict) -> str:
    """Format an issue/PR item as a markdown table row."""
    state = item["state"]
    item_type = "PR" if is_pull_request(item) else "Issue"
    interactions = ", ".join(item.get("_interactions", []))
    title = escape_markdown(item["title"])
    # Truncate long titles
    if len(title) > TITLE_TRUNCATE_LENGTH:
        title = title[:TITLE_TRUNCATE_LENGTH - 3] + "..."
    url = item["url"]
    return f"| {item_type} | {state} | [{title}]({url}) | {interactions} |"


def process_item_for_users(item: dict, usernames: list[str], since_dt: datetime, end_dt: datetime) -> dict:
    """Process an item for multiple users and combine their interactions."""
    all_interactions = []
    latest_interaction = None
    for username in item.get("_usernames", []):
        processed = process_item(item.copy(), username, since_dt, end_dt)
        interactions = processed.get("_interactions", [])
        item_latest = processed.get("_latest_interaction")
        # Tag interactions with username if multiple users
        if len(usernames) > 1:
            for interaction in interactions:
                tagged = f"{interaction} (@{username})"
                if tagged not in all_interactions:
                    all_interactions.append(tagged)
        else:
            for interaction in interactions:
                if interaction not in all_interactions:
                    all_interactions.append(interaction)
        if item_latest and (latest_interaction is None or item_latest > latest_interaction):
            latest_interaction = item_latest
    item["_interactions"] = all_interactions
    item["_latest_interaction"] = latest_interaction
    return item


def compute_user_stats(active_items: list[dict], usernames: list[str]) -> dict[str, UserStats]:
    """Compute user statistics from processed active items.

    Args:
        active_items: List of processed items with _interactions
        usernames: List of usernames to compute stats for

    Returns: {username: UserStats}
    """
    user_stats: dict[str, UserStats] = {username: UserStats() for username in usernames}

    for item in active_items:
        for interaction in item.get("_interactions", []):
            base_interaction, username = parse_interaction(
                interaction,
                usernames[0] if len(usernames) == 1 else None
            )
            if username is None:
                continue
            if username not in user_stats:
                user_stats[username] = UserStats()

            if base_interaction == "author":
                if is_pull_request(item):
                    user_stats[username].prs_authored += 1
                else:
                    user_stats[username].issues_authored += 1
            elif base_interaction == "reviewer" and is_pull_request(item):
                user_stats[username].prs_reviewed += 1
            elif base_interaction == "commenter":
                if is_pull_request(item):
                    user_stats[username].prs_commented += 1
                else:
                    user_stats[username].issues_commented += 1

    return user_stats


def fetch_activity(
    usernames: list[str],
    orgs: list[str],
    since_dt: datetime,
    end_dt: datetime,
    use_cache: bool = True,
    refresh: bool = False,
    cache_full_items: bool = False,
    debug: bool = False
) -> FetchResult:
    """Fetch and process GitHub activity for given users and orgs.

    Args:
        usernames: List of GitHub usernames
        orgs: List of GitHub organizations
        since_dt: Start of time period
        end_dt: End of time period
        use_cache: If True, read from and write to cache
        refresh: If True, skip cache read but still write to cache
        cache_full_items: If True, cache full items (for non-trend mode); if False, only cache stats
        debug: If True, print progress messages

    Returns: FetchResult with active_items, filtered_items, and user_stats
    """
    # Determine cache key - add suffix for full item caching
    cache_key = get_cache_key(usernames, orgs, since_dt, end_dt)
    if cache_full_items:
        cache_key += "_full"

    # Try to load from cache
    if use_cache and not refresh:
        cached = load_from_cache(cache_key)
        if cached:
            data = cached.get("data", {})

            if cache_full_items:
                # Full items cached - reconstruct with datetime objects
                active_items = data.get("active_items", [])
                filtered_items = data.get("filtered_items", [])
                for item in active_items:
                    if item.get("_latest_interaction"):
                        item["_latest_interaction"] = parse_datetime(item["_latest_interaction"])
                user_stats = compute_user_stats(active_items, usernames)
            else:
                # Stats-only cache - reconstruct UserStats
                active_items = []
                filtered_items = []
                cached_stats = data.get("user_stats", {})
                user_stats = {}
                for username in usernames:
                    if username in cached_stats:
                        s = cached_stats[username]
                        user_stats[username] = UserStats(
                            prs_authored=s.get("prs_authored", 0),
                            prs_reviewed=s.get("prs_reviewed", 0),
                            prs_commented=s.get("prs_commented", 0),
                            issues_authored=s.get("issues_authored", 0),
                            issues_commented=s.get("issues_commented", 0),
                        )
                    else:
                        user_stats[username] = UserStats()

            return FetchResult(active_items, filtered_items, user_stats)

    # Fetch fresh data
    since_date = since_dt.strftime("%Y-%m-%d")
    end_date = end_dt.strftime("%Y-%m-%d")

    if debug:
        orgs_str = ", ".join(orgs)
        users_str = ", ".join(f"@{u}" for u in usernames)
        print(f"Fetching GitHub activity for {users_str} in {orgs_str} from {since_date} to {end_date}...\n")

    # Fetch items from all orgs and users
    all_items: dict[str, dict] = {}
    for org in orgs:
        for username in usernames:
            items = gh_search([
                "--involves", username,
                "--owner", org,
                "--updated", f"{since_date}..{end_date}"
            ])

            # Warn if search results hit the limit
            if len(items) >= SEARCH_LIMIT:
                print(
                    f"Warning: Search for @{username} in {org} hit {SEARCH_LIMIT} limit; "
                    f"results may be incomplete. Consider using a shorter date range.",
                    file=sys.stderr
                )

            for item in items:
                if item["url"] not in all_items:
                    all_items[item["url"]] = item
                    all_items[item["url"]]["_usernames"] = [username]
                elif username not in all_items[item["url"]]["_usernames"]:
                    all_items[item["url"]]["_usernames"].append(username)

    items_list = list(all_items.values())

    if not items_list:
        return FetchResult([], [], {username: UserStats() for username in usernames})

    if debug:
        print(f"Found {len(items_list)} candidates. Fetching interaction details and filtering by date...")

    # Process items in parallel
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = {
            executor.submit(process_item_for_users, item, usernames, since_dt, end_dt): item
            for item in items_list
        }
        processed_items = []
        for future in as_completed(futures):
            try:
                processed_items.append(future.result())
            except Exception as e:
                item = futures[future]
                print(f"Warning: Failed to process {item.get('url', 'unknown')}: {e}", file=sys.stderr)

    # Filter out passive-only interactions
    active_items = []
    filtered_items = []
    for item in processed_items:
        interactions = item.get("_interactions", [])
        base_interactions = {parse_interaction(interaction)[0] for interaction in interactions}

        is_passive_only = base_interactions.issubset(PASSIVE_INTERACTION_TYPES)
        has_recent_interaction = item.get("_latest_interaction") is not None

        if not is_passive_only and has_recent_interaction:
            active_items.append(item)
        else:
            filtered_items.append(item)

    # Sort by latest user interaction descending
    active_items.sort(key=lambda x: x["_latest_interaction"], reverse=True)

    # Compute user statistics
    user_stats = compute_user_stats(active_items, usernames)

    # Save to cache
    if use_cache:
        cache_params = {
            "usernames": usernames,
            "orgs": orgs,
            "since_date": since_dt.strftime("%Y-%m-%d"),
            "end_date": end_dt.strftime("%Y-%m-%d"),
        }

        if cache_full_items:
            # Cache full items (serialize datetime)
            def serialize_item(item):
                item_copy = item.copy()
                if item_copy.get("_latest_interaction"):
                    item_copy["_latest_interaction"] = item_copy["_latest_interaction"].isoformat()
                return item_copy

            cache_data = {
                "active_items": [serialize_item(item) for item in active_items],
                "filtered_items": [serialize_item(item) for item in filtered_items],
            }
        else:
            # Cache stats only
            cache_data = {
                "user_stats": {
                    username: {
                        "prs_authored": stats.prs_authored,
                        "prs_reviewed": stats.prs_reviewed,
                        "prs_commented": stats.prs_commented,
                        "issues_authored": stats.issues_authored,
                        "issues_commented": stats.issues_commented,
                    }
                    for username, stats in user_stats.items()
                }
            }

        save_to_cache(cache_key, cache_params, cache_data)

    return FetchResult(active_items, filtered_items, user_stats)


def collect_stats_for_period(
    usernames: list[str],
    orgs: list[str],
    since_dt: datetime,
    end_dt: datetime,
    use_cache: bool = True,
    refresh: bool = False
) -> dict[str, UserStats]:
    """Collect activity stats for a given time period.

    Args:
        usernames: List of GitHub usernames
        orgs: List of GitHub organizations
        since_dt: Start of time period
        end_dt: End of time period
        use_cache: If True, read from and write to cache
        refresh: If True, skip cache read but still write to cache

    Returns: {username: UserStats}
    """
    result = fetch_activity(
        usernames, orgs, since_dt, end_dt,
        use_cache=use_cache,
        refresh=refresh,
        cache_full_items=False  # Stats-only cache for trend mode
    )
    return result.user_stats


def collect_weekly_stats(
    usernames: list[str],
    orgs: list[str],
    num_weeks: int,
    use_cache: bool = True,
    refresh: bool = False
) -> dict[str, dict[str, UserStats]]:
    """Collect stats for each of the last N weeks.

    Args:
        usernames: List of GitHub usernames
        orgs: List of GitHub organizations
        num_weeks: Number of weeks to fetch
        use_cache: If True, read from and write to cache
        refresh: If True, skip cache read but still write to cache

    Returns: {week_label: {username: UserStats}}
    Week labels are formatted as "MM/DD" (start of week - Monday)
    """
    # Clear API cache between runs to avoid stale data
    _api_cache.clear()

    weekly_data: dict[str, dict[str, UserStats]] = {}

    # Calculate weeks going backwards from last complete week (excluding current incomplete week)
    today = datetime.now(timezone.utc)
    # Find the Monday of the last complete week
    days_since_monday = today.weekday()
    last_complete_monday = today - timedelta(days=days_since_monday + 7)
    last_complete_monday = last_complete_monday.replace(hour=0, minute=0, second=0, microsecond=0)

    for week_offset in range(num_weeks - 1, -1, -1):
        week_start = last_complete_monday - timedelta(weeks=week_offset)
        week_end = week_start + timedelta(days=6, hours=23, minutes=59, seconds=59)

        week_label = week_start.strftime("%m/%d")

        # Check if we'll use cache for this week
        cache_key = get_cache_key(usernames, orgs, week_start, week_end)
        will_use_cache = use_cache and not refresh and load_from_cache(cache_key) is not None

        if will_use_cache:
            print(f"  Week {week_label}...", end=" ", flush=True)
        else:
            print(f"  Fetching week {week_label}...", end=" ", flush=True)

        stats = collect_stats_for_period(usernames, orgs, week_start, week_end, use_cache, refresh)
        weekly_data[week_label] = stats

        # Show totals for this week
        total = sum(s.total() for s in stats.values())
        suffix = " (cached)" if will_use_cache else ""
        print(f"({total} activities){suffix}")

    return weekly_data


def get_metric_value(stats: UserStats, metric: str) -> int:
    """Get the value for a specific metric from UserStats.

    Raises:
        ValueError: If metric is not a valid metric name.
    """
    metric_accessors = {
        "prs-authored": lambda s: s.prs_authored,
        "prs-reviewed": lambda s: s.prs_reviewed,
        "prs-commented": lambda s: s.prs_commented,
        "issues-authored": lambda s: s.issues_authored,
        "issues-commented": lambda s: s.issues_commented,
        "total": lambda s: s.total(),
    }

    if metric not in metric_accessors:
        raise ValueError(f"Unknown metric: {metric}. Valid metrics: {', '.join(VALID_METRICS)}")

    return metric_accessors[metric](stats)


def render_trend_charts(
    weekly_data: dict[str, dict[str, UserStats]],
    usernames: list[str],
    metrics: list[str],
    by_user: bool = False
) -> None:
    """Render ASCII line charts using plotext.

    Args:
        weekly_data: Weekly statistics by user
        usernames: List of usernames
        metrics: List of metrics to chart
        by_user: If True, show separate lines per user; if False, show totals only
    """
    if not PLOTEXT_AVAILABLE:
        print("Error: plotext is required for trend charts. Install with: pip install plotext")
        sys.exit(1)

    weeks = list(weekly_data.keys())

    metric_titles = {
        "prs-authored": "PRs Authored",
        "prs-reviewed": "PRs Reviewed",
        "prs-commented": "PRs Commented",
        "issues-authored": "Issues Authored",
        "issues-commented": "Issues Commented",
        "total": "Total Engagements",
    }

    # Use numeric x-axis with week labels as xticks
    x_values = list(range(len(weeks)))

    for metric in metrics:
        if metric not in VALID_METRICS:
            continue

        title = metric_titles.get(metric, metric)

        plt.clear_figure()
        plt.plotsize(80, 15)
        plt.theme("dark")
        plt.canvas_color("default")  # Use terminal background (transparent)
        plt.axes_color("default")    # Make axes area transparent too
        plt.title(title)

        # Calculate values and track max for y-axis scaling
        all_values = []
        if by_user and len(usernames) > 1:
            # Show separate lines per user
            for username in usernames:
                values = [get_metric_value(weekly_data[week].get(username, UserStats()), metric) for week in weeks]
                all_values.extend(values)
                plt.plot(x_values, values, label=username, marker="braille")
        else:
            # Show totals (aggregate all users)
            values = [
                sum(get_metric_value(weekly_data[week].get(u, UserStats()), metric) for u in usernames)
                for week in weeks
            ]
            all_values = values
            plt.plot(x_values, values, marker="braille")

        # Set integer y-ticks with 3-digit minimum width
        max_val = max(all_values) if all_values else 0
        if max_val <= 5:
            ytick_values = list(range(0, max_val + 2))
        else:
            step = max(1, (max_val + 4) // 5)
            ytick_values = list(range(0, max_val + step + 1, step))
        ytick_labels = [f"{v:3d}" for v in ytick_values]
        plt.yticks(ytick_values, ytick_labels)

        plt.xticks(x_values, weeks)
        plt.xlabel("Week")
        plt.ylabel("Count")

        plt.show()
        print()  # Add spacing between charts


def parse_arguments() -> argparse.Namespace:
    """Parse and validate command-line arguments."""
    parser = argparse.ArgumentParser(
        description="Fetch GitHub issues and PRs a user has interacted with in an organization."
    )
    parser.add_argument(
        "--username", "-u",
        help="GitHub username(s) to search for (comma-separated)"
    )
    parser.add_argument(
        "--org", "-o",
        required=True,
        help="GitHub organization(s) to search in (comma-separated)"
    )
    parser.add_argument(
        "--days", "-d",
        type=int,
        default=7,
        help="Number of days to look back (default: 7)"
    )
    parser.add_argument(
        "--end-date", "-e",
        type=parse_date_arg,
        default=None,
        help="End date for the search window (format: YYYY-MM-DD, default: today)"
    )
    parser.add_argument(
        "--last-week",
        action="store_true",
        help="Search the last complete week (Monday to Sunday)"
    )
    parser.add_argument(
        "--show-filtered",
        action="store_true",
        help="Show filtered out items in a separate section"
    )
    parser.add_argument(
        "--trend", "-t",
        action="store_true",
        help="Enable trend mode: analyze multiple weeks and display charts"
    )
    parser.add_argument(
        "--weeks", "-w",
        type=int,
        default=4,
        help="Number of weeks to analyze in trend mode (default: 4)"
    )
    parser.add_argument(
        "--metrics", "-m",
        type=str,
        default=None,
        help="Comma-separated metrics to chart: prs-authored,prs-reviewed,prs-commented,issues-authored,issues-commented,total (default: all)"
    )
    parser.add_argument(
        "--by-user",
        action="store_true",
        help="In trend mode, show separate chart lines per user instead of totals"
    )
    parser.add_argument(
        "--refresh",
        action="store_true",
        help="Force refresh: fetch fresh data and update cache"
    )
    parser.add_argument(
        "--no-cache",
        action="store_true",
        help="Skip cache entirely: don't read from or write to cache"
    )
    parser.add_argument(
        "--debug",
        action="store_true",
        help="Show verbose output (progress messages, per-repo details)"
    )
    parser.add_argument(
        "--clear-cache",
        action="store_true",
        help="Clear all cached data and exit"
    )
    parser.add_argument(
        "--health",
        action="store_true",
        help="Enable health stats mode: analyze repository health metrics"
    )
    parser.add_argument(
        "--repos",
        type=str,
        default=None,
        help="Comma-separated repos (owner/repo). Defaults to all in --org"
    )
    parser.add_argument(
        "--health-metrics",
        type=str,
        default=None,
        help="Comma-separated metrics to display (default: all)"
    )
    parser.add_argument(
        "--ignore-users",
        type=str,
        default=None,
        help="Comma-separated usernames to treat as bots for response time"
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="Preview API calls without executing them (useful for debugging and rate limit planning)"
    )
    return parser.parse_args()


def calculate_date_range(args: argparse.Namespace) -> tuple[datetime, datetime]:
    """Calculate the date range based on arguments."""
    if args.last_week:
        return get_last_complete_week()
    elif args.end_date:
        end_dt = args.end_date.replace(hour=23, minute=59, second=59)
        since_dt = end_dt - timedelta(days=args.days)
        return since_dt, end_dt
    else:
        end_dt = datetime.now(timezone.utc)
        since_dt = end_dt - timedelta(days=args.days)
        return since_dt, end_dt


def parse_user_inputs(args: argparse.Namespace) -> tuple[list[str], list[str]]:
    """Parse and validate usernames and organizations."""
    # Handle optional username (e.g., in health mode)
    usernames = []
    if args.username:
        usernames = [u.strip() for u in args.username.split(",") if u.strip()]
        if not usernames:
            print("Error: No valid usernames provided", file=sys.stderr)
            sys.exit(1)

    orgs = [o.strip() for o in args.org.split(",") if o.strip()]
    if not orgs:
        print("Error: No valid organizations provided", file=sys.stderr)
        sys.exit(1)

    return usernames, orgs


def run_trend_mode(
    args: argparse.Namespace,
    usernames: list[str],
    orgs: list[str]
) -> None:
    """Handle trend mode execution."""
    if not PLOTEXT_AVAILABLE:
        print("Error: plotext is required for trend charts. Install with: pip install plotext")
        sys.exit(1)

    # Parse and validate metrics
    if args.metrics:
        metrics = [m.strip() for m in args.metrics.split(",")]
        invalid = set(metrics) - VALID_METRICS
        if invalid:
            print(f"Error: Invalid metrics: {', '.join(invalid)}")
            print(f"Valid metrics: {', '.join(ALL_METRICS)}")
            sys.exit(1)
    else:
        metrics = ALL_METRICS

    use_cache = not args.no_cache
    cache_status = ""
    if args.no_cache:
        cache_status = " (cache disabled)"
    elif args.refresh:
        cache_status = " (refreshing cache)"

    users_str = ", ".join(f"@{u}" for u in usernames)
    orgs_str = ", ".join(orgs)
    print(f"Fetching {args.weeks}-week trend for {users_str} in {orgs_str}{cache_status}...\n")

    weekly_data = collect_weekly_stats(usernames, orgs, args.weeks, use_cache, args.refresh)

    print(f"\n## Trend Charts\n")
    render_trend_charts(weekly_data, usernames, metrics, by_user=args.by_user)

    # Print summary table
    print("## Weekly Summary\n")
    weeks = list(weekly_data.keys())
    headers = ["Week"] + [m.replace("-", " ").title() for m in metrics]

    # Build all rows first to calculate column widths
    rows = []
    for week in weeks:
        values = [week]
        for metric in metrics:
            total = sum(
                get_metric_value(weekly_data[week].get(username, UserStats()), metric)
                for username in usernames
            )
            values.append(str(total))
        rows.append(values)

    # Calculate column widths
    col_widths = [len(h) for h in headers]
    for row in rows:
        col_widths = [max(cw, len(val)) for cw, val in zip(col_widths, row)]

    # Print header row
    header_row = "| " + " | ".join(h.ljust(col_widths[i]) for i, h in enumerate(headers)) + " |"
    print(header_row)

    # Print separator row
    separator = "|" + "|".join("-" * (w + 2) for w in col_widths) + "|"
    print(separator)

    # Print data rows (first column left-aligned, rest right-aligned for numbers)
    for row in rows:
        formatted_values = [row[0].ljust(col_widths[0])]  # Week label left-aligned
        formatted_values += [v.rjust(w) for v, w in zip(row[1:], col_widths[1:])]  # Numbers right-aligned
        formatted_row = "| " + " | ".join(formatted_values) + " |"
        print(formatted_row)


def run_health_mode(
    args: argparse.Namespace,
    usernames: list[str],
    orgs: list[str]
) -> None:
    """Handle health mode execution using orchestration functions with caching."""
    # Parse ignore users if provided
    ignore_users = set()
    if args.ignore_users:
        ignore_users = {u.strip() for u in args.ignore_users.split(",") if u.strip()}

    # Parse health metrics to display
    display_metrics = HEALTH_METRICS
    if args.health_metrics:
        display_metrics = [m.strip() for m in args.health_metrics.split(",")]
        invalid = set(display_metrics) - VALID_HEALTH_METRICS
        if invalid:
            print(f"Error: Invalid health metrics: {', '.join(invalid)}")
            print(f"Valid metrics: {', '.join(HEALTH_METRICS)}")
            sys.exit(1)

    # Use --weeks parameter for health mode (default to 1 week if not specified)
    num_weeks = getattr(args, 'weeks', 1)

    # Determine repositories to analyze
    all_repos = []
    if args.repos:
        # Parse specific repos
        repo_names = [r.strip() for r in args.repos.split(",") if r.strip()]
        for repo_name in repo_names:
            # For specific repos, we need to fetch their metadata
            try:
                owner, repo = repo_name.split("/", 1)
                repo_data = gh_api_cached(f"repos/{owner}/{repo}")
                if repo_data:
                    all_repos.append({
                        "id": repo_data["id"],
                        "node_id": repo_data["node_id"],
                        "name": repo_data["name"],
                        "full_name": repo_data["full_name"]
                    })
            except (ValueError, TypeError):
                print(f"Warning: Invalid repo format '{repo_name}', expected 'owner/repo'", file=sys.stderr)
                continue
    else:
        # Fetch all repos from organizations
        for org in orgs:
            org_repos = list_org_repos(org)
            all_repos.extend(org_repos)

    if not all_repos:
        print("Error: No repositories to analyze", file=sys.stderr)
        sys.exit(1)

    # Show initial status
    if num_weeks == 1:
        week_start, week_end = get_last_complete_week()
        print(f"Analyzing health metrics for {len(all_repos)} repositories...")
        if args.debug:
            print(f"Week: {week_start.strftime('%Y-%m-%d')} to {week_end.strftime('%Y-%m-%d')}")
            print(f"Repos: {', '.join([r['full_name'] for r in all_repos[:5]])}{'...' if len(all_repos) > 5 else ''}")
    else:
        orgs_str = ", ".join(orgs)
        print(f"Analyzing {num_weeks}-week health trends for {len(all_repos)} repositories in {orgs_str}...")

    # Collect health metrics using orchestration functions
    results_by_repo = collect_health_for_repos(
        repos=all_repos,
        num_weeks=num_weeks,
        ignore_users=ignore_users,
        refresh=args.refresh,
        dry_run=args.dry_run
    )

    if args.dry_run:
        print("\nDry run completed. No data was fetched or processed.")
        return

    if not results_by_repo:
        print("No health data could be collected.")
        return

    # Filter out repos with no data
    repos_with_data = {repo_name: metrics_list for repo_name, metrics_list in results_by_repo.items() if metrics_list}

    if not repos_with_data:
        print("No health data available for the requested time period.")
        return

    print(f"\nCollected health data for {len(repos_with_data)} repositories.")

    if num_weeks == 1:
        # Single week: display detailed table
        display_single_week_health_table(repos_with_data, display_metrics)
    else:
        # Multiple weeks: display trends
        display_health_trends(repos_with_data, display_metrics, num_weeks)


def display_single_week_health_table(repos_data: dict[str, list[RepoHealthMetrics]], display_metrics: list[str]) -> None:
    """Display health metrics for a single week in table format."""
    # Get the most recent metrics for each repo
    latest_metrics = {}
    week_range = None

    for repo_name, metrics_list in repos_data.items():
        if metrics_list:
            latest_metrics[repo_name] = metrics_list[0]  # Most recent first
            if week_range is None:
                week_range = (metrics_list[0].week_start, metrics_list[0].week_end)

    if not latest_metrics:
        print("No current week data available.")
        return

    # Print header
    week_start, week_end = week_range
    print(f"\n## Repository Health Metrics")
    print(f"Week: {week_start.strftime('%Y-%m-%d')} to {week_end.strftime('%Y-%m-%d')}\n")

    # Define all possible headers and their corresponding metric names
    all_headers_map = {
        "open-issues": "Open Issues",
        "open-prs": "Open PRs",
        "new-issues": "New Issues",
        "new-prs": "New PRs",
        "days-since-release": "Days Since Release",
        "avg-issue-response": "Avg Issue Response (hrs)",
        "avg-pr-response": "Avg PR Response (hrs)",
        "avg-pr-cycle": "Avg PR Cycle (hrs)"
    }

    # Filter headers based on requested metrics
    headers = ["Repository"] + [all_headers_map[m] for m in display_metrics if m in all_headers_map]

    # Sort repositories by name
    sorted_repos = sorted(latest_metrics.items(), key=lambda x: x[0])

    # Print table headers
    print("| " + " | ".join(headers) + " |")
    print("|" + "|".join("-" * (len(h) + 2) for h in headers) + "|")

    # Print data rows
    for repo_name, metrics in sorted_repos:
        values = [repo_name]

        for metric in display_metrics:
            formatter = HEALTH_METRIC_FORMATTERS.get(metric)
            formatted_value = formatter(metrics) if formatter else "N/A"
            values.append(formatted_value)

        print("| " + " | ".join(values) + " |")

    # Calculate and display Total/Average row
    if sorted_repos:
        total_values = ["**Total/Average**"]

        # Define calculation methods for each metric type
        count_metrics = {"open-issues", "open-prs", "new-issues", "new-prs"}
        time_metrics = {"avg-issue-response", "avg-pr-response", "avg-pr-cycle"}

        for metric in display_metrics:
            if metric in count_metrics:
                # Sum for count metrics
                total = sum(getattr(metrics, metric.replace("-", "_")) for _, metrics in sorted_repos)
                total_values.append(f"**{total}**")
            elif metric in time_metrics:
                # Weighted average for time metrics using sample sizes
                field_name = metric.replace("-", "_") + "_hours"

                # Get sample size field name based on metric type
                if metric == "avg-issue-response":
                    sample_size_field = "issue_response_sample_size"
                elif metric == "avg-pr-response":
                    sample_size_field = "pr_response_sample_size"
                elif metric == "avg-pr-cycle":
                    sample_size_field = "merged_pr_count"
                else:
                    sample_size_field = None

                # Calculate weighted average
                weighted_sum = 0
                total_weight = 0

                for _, metrics in sorted_repos:
                    value = getattr(metrics, field_name)
                    sample_size = getattr(metrics, sample_size_field) if sample_size_field else 1

                    if value is not None and sample_size > 0:
                        weighted_sum += value * sample_size
                        total_weight += sample_size

                avg_value = weighted_sum / total_weight if total_weight > 0 else None
                formatted = f"**{format_duration(avg_value)}**" if avg_value is not None else "**-**"
                total_values.append(formatted)
            elif metric == "days-since-release":
                # Average for days since release (only repos with releases)
                values_with_data = [
                    metrics.days_since_release
                    for _, metrics in sorted_repos
                    if metrics.days_since_release is not None
                ]
                avg_value = sum(values_with_data) / len(values_with_data) if values_with_data else None
                formatted = f"**{avg_value:.0f}d**" if avg_value is not None else "**-**"
                total_values.append(formatted)
            else:
                total_values.append("**-**")

        print("| " + " | ".join(total_values) + " |")


def display_health_trends(repos_data: dict[str, list[RepoHealthMetrics]], display_metrics: list[str], num_weeks: int) -> None:
    """Display health trends over multiple weeks."""
    if not PLOTEXT_AVAILABLE:
        print("Error: plotext is required for health trend charts. Install with: pip install plotext")
        return

    # Collect all weeks represented in the data, using full date for sorting
    all_weeks = set()
    for metrics_list in repos_data.values():
        for metrics in metrics_list:
            # Store full date for proper sorting across years
            all_weeks.add(metrics.week_start.date())

    # Sort by full date, then create display labels
    sorted_week_dates = sorted(list(all_weeks))
    weeks = [date.strftime("%m/%d") for date in sorted_week_dates]
    week_date_lookup = {date.strftime("%m/%d"): date for date in sorted_week_dates}
    if len(weeks) < num_weeks:
        print(f"Warning: Only {len(weeks)} weeks of data available (requested {num_weeks})")

    print(f"\n## Health Trends - Last {len(weeks)} Weeks\n")

    # Create charts for each requested metric
    metric_titles = {
        "open-issues": "Open Issues",
        "open-prs": "Open PRs",
        "new-issues": "New Issues",
        "new-prs": "New PRs",
        "days-since-release": "Days Since Last Release",
        "avg-issue-response": "Avg Issue Response Time (hours)",
        "avg-pr-response": "Avg PR Response Time (hours)",
        "avg-pr-cycle": "Avg PR Cycle Time (hours)"
    }

    # Use numeric x-axis with week labels as xticks
    x_values = list(range(len(weeks)))

    for metric in display_metrics:
        if metric not in metric_titles:
            continue

        title = metric_titles[metric]

        plt.clear_figure()
        plt.plotsize(80, 15)
        plt.theme("dark")
        plt.canvas_color("default")
        plt.axes_color("default")
        plt.title(title)

        # Calculate aggregate values across all repos for each week
        aggregate_values = []
        for week in weeks:
            total_value = 0
            count = 0

            for repo_name, metrics_list in repos_data.items():
                for metrics in metrics_list:
                    # Use exact date comparison instead of formatted string
                    if metrics.week_start.date() == week_date_lookup[week]:
                        value = get_health_metric_value(metrics, metric)
                        if value is not None:
                            total_value += value
                            count += 1
                        break

            # Calculate average for this week
            if count > 0:
                if metric in ["avg-issue-response", "avg-pr-response", "avg-pr-cycle"]:
                    aggregate_values.append(total_value / count)  # Average of averages
                else:
                    aggregate_values.append(total_value)  # Sum for counts
            else:
                aggregate_values.append(0)

        # Plot the trend
        plt.plot(x_values, aggregate_values, marker="braille")

        # Set appropriate y-axis ticks
        max_val = max(aggregate_values) if aggregate_values else 0
        if max_val <= 5:
            ytick_values = list(range(0, int(max_val) + 2))
        else:
            step = max(1, int((max_val + 4) // 5))
            ytick_values = list(range(0, int(max_val) + step + 1, step))
        ytick_labels = [f"{v:3d}" if v == int(v) else f"{v:3.1f}" for v in ytick_values]
        plt.yticks(ytick_values, ytick_labels)

        plt.xticks(x_values, weeks)
        plt.xlabel("Week")
        plt.ylabel("Count" if metric not in ["avg-issue-response", "avg-pr-response", "avg-pr-cycle"] else "Hours")

        plt.show()
        print()


def get_health_metric_value(metrics: RepoHealthMetrics, metric: str) -> Optional[float]:
    """Extract a specific metric value from RepoHealthMetrics."""
    extractor = HEALTH_METRIC_EXTRACTORS.get(metric)
    return extractor(metrics) if extractor else None


def print_detailed_results(
    active_items: list[dict],
    filtered_items: list[dict],
    usernames: list[str]
) -> None:
    """Print detailed per-repository results."""
    if len(usernames) == 1:
        # Single user: group by repository
        by_repo = group_by_repository(active_items)

        print(f"\n## Active Participation\n")
        print(f"Found {len(active_items)} issues/PRs with active participation across {len(by_repo)} repositories")
        if filtered_items:
            print(f"(filtered out {len(filtered_items)} items with only passive/old interactions)\n")

        for repo_name in sorted(by_repo.keys()):
            repo_items = by_repo[repo_name]
            print(f"\n### {repo_name} ({len(repo_items)} items)\n")
            print("| Type | State | Title | Interactions |")
            print("|------|-------|-------|--------------|")
            for item in repo_items:
                print(format_table_row(item))
    else:
        # Multiple users: group by user, then by repository
        print(f"\n## Active Participation\n")
        print(f"Found {len(active_items)} unique issues/PRs with active participation")
        if filtered_items:
            print(f"(filtered out {len(filtered_items)} items with only passive/old interactions)\n")

        for username in usernames:
            # Filter items for this user with active interactions
            user_items = []
            for item in active_items:
                user_interactions = [
                    i for i in item.get("_interactions", [])
                    if f"(@{username})" in i
                ]
                if not user_interactions:
                    continue

                # Check if user has active (non-passive) interactions
                base_interactions = {parse_interaction(i)[0] for i in user_interactions}
                if not base_interactions or base_interactions.issubset(PASSIVE_INTERACTION_TYPES):
                    continue

                item_copy = item.copy()
                item_copy["_interactions"] = user_interactions
                user_items.append(item_copy)

            if not user_items:
                continue

            # Group by repository
            by_repo = group_by_repository(user_items)

            print(f"\n## @{username}\n")
            print(f"Found {len(user_items)} issues/PRs across {len(by_repo)} repositories\n")

            for repo_name in sorted(by_repo.keys()):
                repo_items = by_repo[repo_name]
                print(f"\n### {repo_name} ({len(repo_items)} items)\n")
                print("| Type | State | Title | Interactions |")
                print("|------|-------|-------|--------------|")
                for item in repo_items:
                    print(format_table_row(item))


def print_summary_table(user_stats: dict[str, UserStats]) -> None:
    """Print the summary statistics table."""
    print(f"\n## Summary\n")

    # Define column headers
    headers = ["Username", "PRs Authored", "PRs Reviewed", "PRs Commented", "Issues Authored", "Issues Commented", "Total"]

    # Sort by total activity (sum of all interactions) descending
    sorted_users = sorted(
        user_stats.items(),
        key=lambda x: x[1].total(),
        reverse=True
    )

    # Calculate column widths
    col_widths = [len(h) for h in headers]
    for username, stats in sorted_users:
        values = [username] + [str(v) for v in stats.as_list()] + [str(stats.total())]
        col_widths = [max(cw, len(val)) for cw, val in zip(col_widths, values)]

    # Print header row
    header_row = "| " + " | ".join(h.ljust(col_widths[i]) for i, h in enumerate(headers)) + " |"
    print(header_row)

    # Print separator row
    separator = "|" + "|".join("-" * (w + 2) for w in col_widths) + "|"
    print(separator)

    # Print data rows (username left-aligned, numbers right-aligned)
    for username, stats in sorted_users:
        values = [username] + [str(v) for v in stats.as_list()] + [str(stats.total())]
        formatted_values = [values[0].ljust(col_widths[0])]  # Username left-aligned
        formatted_values += [v.rjust(w) for v, w in zip(values[1:], col_widths[1:])]  # Numbers right-aligned
        row = "| " + " | ".join(formatted_values) + " |"
        print(row)

    # Print totals row
    if sorted_users:
        # Calculate totals for each column
        totals = [0] * 5  # 5 stat columns
        for _, stats in sorted_users:
            stats_list = stats.as_list()
            totals = [t + s for t, s in zip(totals, stats_list)]

        # Print separator before totals
        print(separator)

        # Print totals row, including a grand total (sum of all totals)
        grand_total = sum(totals)
        total_values = ["**Total**"] + [str(v) for v in totals] + [str(grand_total)]
        formatted_totals = [total_values[0].ljust(col_widths[0])]  # Label left-aligned
        formatted_totals += [v.rjust(w) for v, w in zip(total_values[1:], col_widths[1:])]  # Numbers right-aligned
        total_row = "| " + " | ".join(formatted_totals) + " |"
        print(total_row)


def print_filtered_items(filtered_items: list[dict]) -> None:
    """Print the filtered items section."""
    print(f"\n## Filtered Out ({len(filtered_items)} items)\n")
    print("Items with only passive interactions (review-requested, assignee, mentioned) or no recent activity:\n")

    # Group filtered by repository
    filtered_by_repo = group_by_repository(filtered_items)

    for repo_name in sorted(filtered_by_repo.keys()):
        repo_items = filtered_by_repo[repo_name]
        print(f"\n### {repo_name} ({len(repo_items)} items)\n")
        print("| Type | State | Title | Interactions |")
        print("|------|-------|-------|--------------|")
        for item in repo_items:
            print(format_table_row(item))


def main() -> None:
    """Main entry point for the script."""
    args = parse_arguments()

    # Handle --clear-cache early (doesn't require gh CLI)
    if args.clear_cache:
        count = clear_cache()
        print(f"Cleared {count} cached file(s) from {get_cache_dir()}")
        return

    # Validate gh CLI is installed and authenticated
    validate_gh_cli()

    # Validate argument dependencies
    if not args.health and not args.username:
        print("Error: --username is required for normal and trend modes", file=sys.stderr)
        sys.exit(1)

    # Parse inputs and calculate date range
    since_dt, end_dt = calculate_date_range(args)
    usernames, orgs = parse_user_inputs(args)

    # Handle trend mode
    if args.trend:
        if not usernames:
            print("Error: No valid usernames provided for trend analysis", file=sys.stderr)
            sys.exit(1)
        run_trend_mode(args, usernames, orgs)
        return

    # Handle health mode
    if args.health:
        run_health_mode(args, usernames, orgs)
        return

    # For normal activity mode, ensure we have usernames
    if not usernames:
        print("Error: No valid usernames provided for activity search", file=sys.stderr)
        sys.exit(1)

    # Fetch activity using shared function
    use_cache = not args.no_cache
    result = fetch_activity(
        usernames, orgs, since_dt, end_dt,
        use_cache=use_cache,
        refresh=args.refresh,
        cache_full_items=True,
        debug=args.debug
    )

    if not result.active_items and not result.filtered_items:
        print("No activity found.")
        return

    # Print detailed results (only in debug mode)
    if args.debug:
        print_detailed_results(result.active_items, result.filtered_items, usernames)

    # Print summary table
    print_summary_table(result.user_stats)

    # Show filtered items if requested
    if args.show_filtered and result.filtered_items:
        print_filtered_items(result.filtered_items)


if __name__ == "__main__":
    main()
