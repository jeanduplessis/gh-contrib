#!/usr/bin/env python3
"""
Fetch GitHub issues and PRs a user has interacted with in a given organization.

Uses the gh CLI for authentication.

Usage:
    ./gh-contrib --username <username> --org <org> [--days <days>]

Requirements:
    - gh CLI installed and authenticated (https://cli.github.com/)
"""

import argparse
import hashlib
import json
import os
import subprocess
import sys
import tempfile
import threading
import time
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass
from datetime import datetime, timedelta, timezone
from pathlib import Path
from typing import Callable, Optional

# Optional import for trend mode
PLOTEXT_AVAILABLE = False
try:
    import plotext as plt
    PLOTEXT_AVAILABLE = True
except ImportError:
    pass

# Constants
SEARCH_LIMIT = 1000  # GitHub's maximum for search
MAX_WORKERS = 10
TITLE_TRUNCATE_LENGTH = 60
API_PAGE_SIZE = 100
SUBPROCESS_TIMEOUT = 30  # seconds
MAX_PAGINATION_PAGES = 10  # Safety limit for pagination
PASSIVE_INTERACTION_TYPES = frozenset({"review-requested", "assignee", "mentioned"})

# Retry configuration
MAX_RETRIES = 3
INITIAL_RETRY_DELAY = 1.0  # seconds
BACKOFF_FACTOR = 2.0
RETRYABLE_ERRORS = frozenset({"rate limit", "502", "503", "504", "timeout"})
VALID_METRICS = frozenset({
    "prs-authored", "prs-reviewed", "prs-commented",
    "issues-authored", "issues-commented", "total"
})
ALL_METRICS = ["prs-authored", "prs-reviewed", "prs-commented", "issues-authored", "issues-commented", "total"]

# Cache version - bump when cache format changes
CACHE_VERSION = 1


def parse_date_arg(value: str) -> datetime:
    """Parse a date argument in YYYY-MM-DD format for argparse."""
    try:
        return datetime.strptime(value, "%Y-%m-%d").replace(tzinfo=timezone.utc)
    except ValueError:
        raise argparse.ArgumentTypeError(f"Invalid date format: '{value}'. Use YYYY-MM-DD")


def get_last_complete_week() -> tuple[datetime, datetime]:
    """Return (start, end) of the last complete Mon-Sun week.

    Returns the Monday 00:00:00 and Sunday 23:59:59 of the most recent
    complete week (not including the current week).
    """
    today = datetime.now(timezone.utc)
    days_since_monday = today.weekday()
    # Last Monday (start of last complete week)
    week_start = today - timedelta(days=days_since_monday + 7)
    week_start = week_start.replace(hour=0, minute=0, second=0, microsecond=0)
    # Last Sunday (end of last complete week)
    week_end = week_start + timedelta(days=6, hours=23, minutes=59, seconds=59)
    return week_start, week_end


def validate_gh_cli() -> None:
    """Validate that gh CLI is installed and authenticated."""
    try:
        subprocess.run(
            ["gh", "auth", "status"],
            capture_output=True,
            check=True,
            timeout=5
        )
    except FileNotFoundError:
        print("Error: GitHub CLI (gh) not found.", file=sys.stderr)
        print("Install from https://cli.github.com/", file=sys.stderr)
        sys.exit(1)
    except subprocess.CalledProcessError:
        print("Error: GitHub CLI not authenticated.", file=sys.stderr)
        print("Run 'gh auth login' to authenticate.", file=sys.stderr)
        sys.exit(1)
    except subprocess.TimeoutExpired:
        print("Error: GitHub CLI auth check timed out.", file=sys.stderr)
        sys.exit(1)


def get_cache_dir() -> Path:
    """Return the cache directory, creating it if needed with secure permissions."""
    cache_dir = Path.home() / ".gh-contrib" / "cache"
    cache_dir.mkdir(parents=True, exist_ok=True, mode=0o700)
    return cache_dir


def get_cache_key(usernames: list[str], orgs: list[str], since_dt: datetime, end_dt: datetime) -> str:
    """Generate a cache key hash from query parameters."""
    sorted_usernames = ",".join(sorted(username.lower() for username in usernames))
    sorted_orgs = ",".join(sorted(org.lower() for org in orgs))
    since_date = since_dt.strftime("%Y-%m-%d")
    end_date = end_dt.strftime("%Y-%m-%d")

    key_parts = [sorted_usernames, sorted_orgs, since_date, end_date]
    combined_key = "|".join(key_parts)
    return hashlib.sha256(combined_key.encode()).hexdigest()[:16]


def load_from_cache(cache_key: str) -> Optional[dict]:
    """Load cached data if it exists and is valid."""
    cache_file = get_cache_dir() / f"{cache_key}.json"
    if not cache_file.exists():
        return None
    try:
        with open(cache_file) as f:
            data = json.load(f)
        if data.get("version") != CACHE_VERSION:
            return None
        return data
    except (json.JSONDecodeError, KeyError):
        return None


def save_to_cache(cache_key: str, params: dict, data: dict) -> None:
    """Save data to cache atomically to prevent corruption."""
    cache_dir = get_cache_dir()
    target_file = cache_dir / f"{cache_key}.json"
    cache_data = {
        "version": CACHE_VERSION,
        "created_at": datetime.now(timezone.utc).isoformat(),
        "params": params,
        "data": data,
    }

    # Write to temp file then atomically rename
    fd, temp_path = tempfile.mkstemp(dir=cache_dir, suffix=".json")
    try:
        with os.fdopen(fd, "w") as f:
            json.dump(cache_data, f, indent=2, default=str)
        os.replace(temp_path, target_file)
    except Exception:
        # Clean up temp file on error
        if os.path.exists(temp_path):
            os.unlink(temp_path)
        raise


def clear_cache() -> int:
    """Clear all cached data. Returns number of files removed."""
    cache_dir = get_cache_dir()
    count = 0
    for cache_file in cache_dir.glob("*.json"):
        cache_file.unlink()
        count += 1
    return count


@dataclass
class UserStats:
    """Statistics for a user's GitHub activity."""
    prs_authored: int = 0
    prs_reviewed: int = 0
    prs_commented: int = 0
    issues_authored: int = 0
    issues_commented: int = 0

    def total(self) -> int:
        """Return total activity count."""
        return sum([
            self.prs_authored,
            self.prs_reviewed,
            self.prs_commented,
            self.issues_authored,
            self.issues_commented
        ])

    def as_list(self) -> list[int]:
        """Return stats as a list for table formatting."""
        return [
            self.prs_authored,
            self.prs_reviewed,
            self.prs_commented,
            self.issues_authored,
            self.issues_commented
        ]


@dataclass
class FetchResult:
    """Result of fetching and processing GitHub activity."""
    active_items: list[dict]
    filtered_items: list[dict]
    user_stats: dict[str, UserStats]


def is_pull_request(item: dict) -> bool:
    """Determine if an item is a pull request."""
    return (
        item.get("isPullRequest", False)
        or item["state"] == "MERGED"
        or "/pull/" in item["url"]
    )


def parse_interaction(interaction: str, default_username: Optional[str] = None) -> tuple[str, Optional[str]]:
    """Parse interaction string into base type and username.

    Args:
        interaction: String like "author (@username)" or "author"
        default_username: Username to use if not tagged

    Returns:
        Tuple of (base_interaction, username)
    """
    if " (@" in interaction:
        base = interaction.split(" (@")[0]
        username = interaction.split(" (@")[1].rstrip(")")
        return base, username
    return interaction, default_username


def group_by_repository(items: list[dict]) -> dict[str, list[dict]]:
    """Group items by repository full name (owner/repo)."""
    by_repo = defaultdict(list)
    for item in items:
        repo_name = item["repository"]["nameWithOwner"]
        by_repo[repo_name].append(item)
    return dict(by_repo)


def gh_search(args: list[str]) -> list[dict]:
    """Execute a gh search command and return results."""
    cmd = ["gh", "search", "issues"] + args + [
        "--include-prs",
        "--json", "url,title,state,repository,updatedAt,createdAt,author,assignees,isPullRequest,number",
        "--limit", str(SEARCH_LIMIT)
    ]

    try:
        result = subprocess.run(cmd, capture_output=True, text=True, check=True, timeout=SUBPROCESS_TIMEOUT)
        return json.loads(result.stdout)
    except subprocess.TimeoutExpired:
        print(f"Error: gh search timed out after {SUBPROCESS_TIMEOUT}s", file=sys.stderr)
        sys.exit(1)
    except subprocess.CalledProcessError as e:
        print(f"Error calling gh search: {e.stderr}", file=sys.stderr)
        sys.exit(1)
    except FileNotFoundError:
        print("Error: gh CLI not found. Install from https://cli.github.com/", file=sys.stderr)
        sys.exit(1)


def gh_api(endpoint: str) -> Optional[list | dict]:
    """Execute a GitHub API call using gh CLI with retry and exponential backoff.

    Retries on rate limits, server errors (502, 503, 504), and timeouts.
    Returns the parsed JSON response, or None on error.
    """
    cmd = ["gh", "api", endpoint]
    delay = INITIAL_RETRY_DELAY

    for attempt in range(MAX_RETRIES + 1):
        try:
            result = subprocess.run(cmd, capture_output=True, text=True, check=True, timeout=SUBPROCESS_TIMEOUT)
            return json.loads(result.stdout)
        except subprocess.TimeoutExpired:
            if attempt < MAX_RETRIES:
                print(f"API call timed out, retrying in {delay}s...", file=sys.stderr)
                time.sleep(delay)
                delay *= BACKOFF_FACTOR
                continue
            print(f"Warning: API call timed out after {MAX_RETRIES} retries: {endpoint}", file=sys.stderr)
            return None
        except subprocess.CalledProcessError as e:
            stderr_lower = e.stderr.lower() if e.stderr else ""
            is_retryable = any(err in stderr_lower for err in RETRYABLE_ERRORS)

            if is_retryable and attempt < MAX_RETRIES:
                if "rate limit" in stderr_lower:
                    print(f"Rate limit hit, waiting {delay}s before retry...", file=sys.stderr)
                else:
                    print(f"Server error, retrying in {delay}s...", file=sys.stderr)
                time.sleep(delay)
                delay *= BACKOFF_FACTOR
                continue

            if "rate limit" in stderr_lower:
                print(f"Warning: GitHub API rate limit exhausted: {endpoint}", file=sys.stderr)
            return None
        except json.JSONDecodeError:
            return None

    return None


class APICache:
    """Thread-safe cache for API responses using singleflight pattern.

    Prevents duplicate concurrent API calls and caches results for the
    duration of script execution.
    """

    def __init__(self):
        self._cache: dict[str, list | dict | None] = {}
        self._pending: dict[str, threading.Event] = {}
        self._lock = threading.Lock()

    def get(self, endpoint: str) -> tuple[bool, list | dict | None]:
        """Get cached value. Returns (found, value)."""
        with self._lock:
            if endpoint in self._cache:
                return True, self._cache[endpoint]
            return False, None

    def set(self, endpoint: str, value: list | dict | None) -> None:
        """Store value in cache."""
        with self._lock:
            self._cache[endpoint] = value

    def clear(self) -> None:
        """Clear all cached data."""
        with self._lock:
            self._cache.clear()
            self._pending.clear()

    def get_or_fetch(
        self,
        endpoint: str,
        fetch_fn: Callable[[str], list | dict | None]
    ) -> list | dict | None:
        """Get from cache or fetch using singleflight pattern.

        If multiple threads request the same endpoint concurrently, only one
        will perform the fetch while others wait for the result.
        """
        with self._lock:
            # Check if result is already cached
            if endpoint in self._cache:
                return self._cache[endpoint]
            # Check if another thread is already fetching this endpoint
            if endpoint in self._pending:
                event = self._pending[endpoint]
            else:
                # We're the first - create event and mark as pending
                event = threading.Event()
                self._pending[endpoint] = event
                event = None  # Signal that we should do the fetch

        if event is not None:
            # Wait for the other thread to complete the fetch
            event.wait()
            with self._lock:
                return self._cache.get(endpoint)

        # We're responsible for fetching
        try:
            result = fetch_fn(endpoint)
            with self._lock:
                self._cache[endpoint] = result
                pending_event = self._pending.pop(endpoint, None)
            if pending_event:
                pending_event.set()
            return result
        except Exception:
            # On error, still need to signal waiting threads and clean up
            with self._lock:
                pending_event = self._pending.pop(endpoint, None)
            if pending_event:
                pending_event.set()
            raise


# Module-level cache instance
_api_cache = APICache()


def gh_api_cached(endpoint: str) -> list | dict | None:
    """Execute a GitHub API call with caching to avoid redundant requests.

    This cache persists for the entire script execution, significantly reducing
    API calls when processing multiple users on the same items.
    """
    return _api_cache.get_or_fetch(endpoint, gh_api)


def parse_datetime(dt_str: str) -> datetime:
    """Parse ISO datetime string to datetime object."""
    # Handle both formats: with and without milliseconds
    dt_str = dt_str.replace("Z", "+00:00")
    return datetime.fromisoformat(dt_str)


def get_interaction_details(item: dict, username: str, since: datetime, until: datetime) -> tuple[list[str], Optional[datetime]]:
    """Determine how the user interacted with this issue/PR and when."""
    interactions = []
    latest_interaction: Optional[datetime] = None
    username_lower = username.lower()

    # Check if user is author (only count if created within date range)
    if item.get("author", {}).get("login", "").lower() == username_lower:
        if created_at := item.get("createdAt"):
            created_dt = parse_datetime(created_at)
            if since <= created_dt <= until:
                interactions.append("author")
                if latest_interaction is None or created_dt > latest_interaction:
                    latest_interaction = created_dt

    # Check if user is assignee (passive, no timestamp to check)
    assignees = item.get("assignees", [])
    if any(a.get("login", "").lower() == username_lower for a in assignees):
        interactions.append("assignee")

    return interactions, latest_interaction


def _fetch_all_pages(endpoint_base: str) -> list:
    """Fetch all pages of a paginated endpoint.

    Args:
        endpoint_base: The base endpoint URL (without query params)

    Returns:
        List of all items across all pages (up to MAX_PAGINATION_PAGES)
    """
    all_results = []

    for page in range(1, MAX_PAGINATION_PAGES + 1):
        endpoint = f"{endpoint_base}?per_page={API_PAGE_SIZE}&page={page}"
        result = gh_api_cached(endpoint)

        if not result or not isinstance(result, list):
            break

        all_results.extend(result)

        # If we got less than a full page, we're done
        if len(result) < API_PAGE_SIZE:
            break

    return all_results


def fetch_additional_interactions(item: dict, username: str, since: datetime, until: datetime) -> tuple[list[str], Optional[datetime]]:
    """Fetch additional interaction details from API (comments, reviews) with timestamps.

    Fetches all pages of comments and reviews to ensure complete data.
    """
    interactions = []
    latest_interaction: Optional[datetime] = None
    username_lower = username.lower()
    repo = item["repository"]["nameWithOwner"]
    number = item["number"]

    # Fetch ALL comments with pagination
    comments = _fetch_all_pages(f"repos/{repo}/issues/{number}/comments")
    if comments:
        user_comments = [
            c for c in comments
            if c.get("user", {}).get("login", "").lower() == username_lower
        ]
        # Filter by date and find latest
        for comment in user_comments:
            comment_dt = parse_datetime(comment["created_at"])
            if since <= comment_dt <= until:
                if "commenter" not in interactions:
                    interactions.append("commenter")
                if latest_interaction is None or comment_dt > latest_interaction:
                    latest_interaction = comment_dt

    # Fetch ALL PR reviews with pagination (if it's a PR)
    if is_pull_request(item):
        reviews = _fetch_all_pages(f"repos/{repo}/pulls/{number}/reviews")
        if reviews:
            user_reviews = [
                r for r in reviews
                if r.get("user", {}).get("login", "").lower() == username_lower
            ]
            # Filter by date and find latest
            for review in user_reviews:
                review_dt = parse_datetime(review["submitted_at"])
                if since <= review_dt <= until:
                    if "reviewer" not in interactions:
                        interactions.append("reviewer")
                    if latest_interaction is None or review_dt > latest_interaction:
                        latest_interaction = review_dt

    return interactions, latest_interaction


def process_item(item: dict, username: str, since: datetime, until: datetime) -> dict:
    """Process a single item to get all interaction details."""
    interactions, latest_basic_interaction = get_interaction_details(item, username, since, until)
    additional, latest_additional_interaction = fetch_additional_interactions(item, username, since, until)
    interactions.extend(additional)

    # Determine the latest interaction timestamp
    latest_interaction = max(
        filter(None, [latest_basic_interaction, latest_additional_interaction]),
        default=None
    )

    # If no specific interaction found, mark as "mentioned" (catch-all for involves)
    if not interactions:
        interactions.append("mentioned")

    item["_interactions"] = interactions
    item["_latest_interaction"] = latest_interaction
    return item


# Markdown special characters that need escaping
MARKDOWN_ESCAPE_CHARS = {
    "|": "\\|",   # Table cell separator
    "[": "\\[",   # Link start
    "]": "\\]",   # Link end
    "`": "\\`",   # Code block
    "*": "\\*",   # Bold/italic
    "_": "\\_",   # Bold/italic
    "#": "\\#",   # Headers
    "~": "\\~",   # Strikethrough
    "<": "&lt;",  # HTML entities
    ">": "&gt;",  # HTML entities
    "\n": " ",    # Newlines (replace, not escape)
    "\r": "",     # Carriage returns (remove)
}


def escape_markdown(text: str) -> str:
    """Escape characters that break markdown tables/links.

    Escapes: | [ ] ` * _ # ~ < >
    Replaces newlines with spaces and removes carriage returns.
    """
    result = text
    for char, replacement in MARKDOWN_ESCAPE_CHARS.items():
        result = result.replace(char, replacement)
    return result


def format_table_row(item: dict) -> str:
    """Format an issue/PR item as a markdown table row."""
    state = item["state"]
    item_type = "PR" if is_pull_request(item) else "Issue"
    interactions = ", ".join(item.get("_interactions", []))
    title = escape_markdown(item["title"])
    # Truncate long titles
    if len(title) > TITLE_TRUNCATE_LENGTH:
        title = title[:TITLE_TRUNCATE_LENGTH - 3] + "..."
    url = item["url"]
    return f"| {item_type} | {state} | [{title}]({url}) | {interactions} |"


def process_item_for_users(item: dict, usernames: list[str], since_dt: datetime, end_dt: datetime) -> dict:
    """Process an item for multiple users and combine their interactions."""
    all_interactions = []
    latest_interaction = None
    for username in item.get("_usernames", []):
        processed = process_item(item.copy(), username, since_dt, end_dt)
        interactions = processed.get("_interactions", [])
        item_latest = processed.get("_latest_interaction")
        # Tag interactions with username if multiple users
        if len(usernames) > 1:
            for interaction in interactions:
                tagged = f"{interaction} (@{username})"
                if tagged not in all_interactions:
                    all_interactions.append(tagged)
        else:
            for interaction in interactions:
                if interaction not in all_interactions:
                    all_interactions.append(interaction)
        if item_latest and (latest_interaction is None or item_latest > latest_interaction):
            latest_interaction = item_latest
    item["_interactions"] = all_interactions
    item["_latest_interaction"] = latest_interaction
    return item


def compute_user_stats(active_items: list[dict], usernames: list[str]) -> dict[str, UserStats]:
    """Compute user statistics from processed active items.

    Args:
        active_items: List of processed items with _interactions
        usernames: List of usernames to compute stats for

    Returns: {username: UserStats}
    """
    user_stats: dict[str, UserStats] = {username: UserStats() for username in usernames}

    for item in active_items:
        for interaction in item.get("_interactions", []):
            base_interaction, username = parse_interaction(
                interaction,
                usernames[0] if len(usernames) == 1 else None
            )
            if username is None:
                continue
            if username not in user_stats:
                user_stats[username] = UserStats()

            if base_interaction == "author":
                if is_pull_request(item):
                    user_stats[username].prs_authored += 1
                else:
                    user_stats[username].issues_authored += 1
            elif base_interaction == "reviewer" and is_pull_request(item):
                user_stats[username].prs_reviewed += 1
            elif base_interaction == "commenter":
                if is_pull_request(item):
                    user_stats[username].prs_commented += 1
                else:
                    user_stats[username].issues_commented += 1

    return user_stats


def fetch_activity(
    usernames: list[str],
    orgs: list[str],
    since_dt: datetime,
    end_dt: datetime,
    use_cache: bool = True,
    refresh: bool = False,
    cache_full_items: bool = False,
    debug: bool = False
) -> FetchResult:
    """Fetch and process GitHub activity for given users and orgs.

    Args:
        usernames: List of GitHub usernames
        orgs: List of GitHub organizations
        since_dt: Start of time period
        end_dt: End of time period
        use_cache: If True, read from and write to cache
        refresh: If True, skip cache read but still write to cache
        cache_full_items: If True, cache full items (for non-trend mode); if False, only cache stats
        debug: If True, print progress messages

    Returns: FetchResult with active_items, filtered_items, and user_stats
    """
    # Determine cache key - add suffix for full item caching
    cache_key = get_cache_key(usernames, orgs, since_dt, end_dt)
    if cache_full_items:
        cache_key += "_full"

    # Try to load from cache
    if use_cache and not refresh:
        cached = load_from_cache(cache_key)
        if cached:
            data = cached.get("data", {})

            if cache_full_items:
                # Full items cached - reconstruct with datetime objects
                active_items = data.get("active_items", [])
                filtered_items = data.get("filtered_items", [])
                for item in active_items:
                    if item.get("_latest_interaction"):
                        item["_latest_interaction"] = parse_datetime(item["_latest_interaction"])
                user_stats = compute_user_stats(active_items, usernames)
            else:
                # Stats-only cache - reconstruct UserStats
                active_items = []
                filtered_items = []
                cached_stats = data.get("user_stats", {})
                user_stats = {}
                for username in usernames:
                    if username in cached_stats:
                        s = cached_stats[username]
                        user_stats[username] = UserStats(
                            prs_authored=s.get("prs_authored", 0),
                            prs_reviewed=s.get("prs_reviewed", 0),
                            prs_commented=s.get("prs_commented", 0),
                            issues_authored=s.get("issues_authored", 0),
                            issues_commented=s.get("issues_commented", 0),
                        )
                    else:
                        user_stats[username] = UserStats()

            return FetchResult(active_items, filtered_items, user_stats)

    # Fetch fresh data
    since_date = since_dt.strftime("%Y-%m-%d")
    end_date = end_dt.strftime("%Y-%m-%d")

    if debug:
        orgs_str = ", ".join(orgs)
        users_str = ", ".join(f"@{u}" for u in usernames)
        print(f"Fetching GitHub activity for {users_str} in {orgs_str} from {since_date} to {end_date}...\n")

    # Fetch items from all orgs and users
    all_items: dict[str, dict] = {}
    for org in orgs:
        for username in usernames:
            items = gh_search([
                "--involves", username,
                "--owner", org,
                "--updated", f"{since_date}..{end_date}"
            ])

            # Warn if search results hit the limit
            if len(items) >= SEARCH_LIMIT:
                print(
                    f"Warning: Search for @{username} in {org} hit {SEARCH_LIMIT} limit; "
                    f"results may be incomplete. Consider using a shorter date range.",
                    file=sys.stderr
                )

            for item in items:
                if item["url"] not in all_items:
                    all_items[item["url"]] = item
                    all_items[item["url"]]["_usernames"] = [username]
                elif username not in all_items[item["url"]]["_usernames"]:
                    all_items[item["url"]]["_usernames"].append(username)

    items_list = list(all_items.values())

    if not items_list:
        return FetchResult([], [], {username: UserStats() for username in usernames})

    if debug:
        print(f"Found {len(items_list)} candidates. Fetching interaction details and filtering by date...")

    # Process items in parallel
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = {
            executor.submit(process_item_for_users, item, usernames, since_dt, end_dt): item
            for item in items_list
        }
        processed_items = []
        for future in as_completed(futures):
            try:
                processed_items.append(future.result())
            except Exception as e:
                item = futures[future]
                print(f"Warning: Failed to process {item.get('url', 'unknown')}: {e}", file=sys.stderr)

    # Filter out passive-only interactions
    active_items = []
    filtered_items = []
    for item in processed_items:
        interactions = item.get("_interactions", [])
        base_interactions = {parse_interaction(interaction)[0] for interaction in interactions}

        is_passive_only = base_interactions.issubset(PASSIVE_INTERACTION_TYPES)
        has_recent_interaction = item.get("_latest_interaction") is not None

        if not is_passive_only and has_recent_interaction:
            active_items.append(item)
        else:
            filtered_items.append(item)

    # Sort by latest user interaction descending
    active_items.sort(key=lambda x: x["_latest_interaction"], reverse=True)

    # Compute user statistics
    user_stats = compute_user_stats(active_items, usernames)

    # Save to cache
    if use_cache:
        cache_params = {
            "usernames": usernames,
            "orgs": orgs,
            "since_date": since_dt.strftime("%Y-%m-%d"),
            "end_date": end_dt.strftime("%Y-%m-%d"),
        }

        if cache_full_items:
            # Cache full items (serialize datetime)
            def serialize_item(item):
                item_copy = item.copy()
                if item_copy.get("_latest_interaction"):
                    item_copy["_latest_interaction"] = item_copy["_latest_interaction"].isoformat()
                return item_copy

            cache_data = {
                "active_items": [serialize_item(item) for item in active_items],
                "filtered_items": [serialize_item(item) for item in filtered_items],
            }
        else:
            # Cache stats only
            cache_data = {
                "user_stats": {
                    username: {
                        "prs_authored": stats.prs_authored,
                        "prs_reviewed": stats.prs_reviewed,
                        "prs_commented": stats.prs_commented,
                        "issues_authored": stats.issues_authored,
                        "issues_commented": stats.issues_commented,
                    }
                    for username, stats in user_stats.items()
                }
            }

        save_to_cache(cache_key, cache_params, cache_data)

    return FetchResult(active_items, filtered_items, user_stats)


def collect_stats_for_period(
    usernames: list[str],
    orgs: list[str],
    since_dt: datetime,
    end_dt: datetime,
    use_cache: bool = True,
    refresh: bool = False
) -> dict[str, UserStats]:
    """Collect activity stats for a given time period.

    Args:
        usernames: List of GitHub usernames
        orgs: List of GitHub organizations
        since_dt: Start of time period
        end_dt: End of time period
        use_cache: If True, read from and write to cache
        refresh: If True, skip cache read but still write to cache

    Returns: {username: UserStats}
    """
    result = fetch_activity(
        usernames, orgs, since_dt, end_dt,
        use_cache=use_cache,
        refresh=refresh,
        cache_full_items=False  # Stats-only cache for trend mode
    )
    return result.user_stats


def collect_weekly_stats(
    usernames: list[str],
    orgs: list[str],
    num_weeks: int,
    use_cache: bool = True,
    refresh: bool = False
) -> dict[str, dict[str, UserStats]]:
    """Collect stats for each of the last N weeks.

    Args:
        usernames: List of GitHub usernames
        orgs: List of GitHub organizations
        num_weeks: Number of weeks to fetch
        use_cache: If True, read from and write to cache
        refresh: If True, skip cache read but still write to cache

    Returns: {week_label: {username: UserStats}}
    Week labels are formatted as "MM/DD" (start of week - Monday)
    """
    # Clear API cache between runs to avoid stale data
    _api_cache.clear()

    weekly_data: dict[str, dict[str, UserStats]] = {}

    # Calculate weeks going backwards from last complete week (excluding current incomplete week)
    today = datetime.now(timezone.utc)
    # Find the Monday of the last complete week
    days_since_monday = today.weekday()
    last_complete_monday = today - timedelta(days=days_since_monday + 7)
    last_complete_monday = last_complete_monday.replace(hour=0, minute=0, second=0, microsecond=0)

    for week_offset in range(num_weeks - 1, -1, -1):
        week_start = last_complete_monday - timedelta(weeks=week_offset)
        week_end = week_start + timedelta(days=6, hours=23, minutes=59, seconds=59)

        week_label = week_start.strftime("%m/%d")

        # Check if we'll use cache for this week
        cache_key = get_cache_key(usernames, orgs, week_start, week_end)
        will_use_cache = use_cache and not refresh and load_from_cache(cache_key) is not None

        if will_use_cache:
            print(f"  Week {week_label}...", end=" ", flush=True)
        else:
            print(f"  Fetching week {week_label}...", end=" ", flush=True)

        stats = collect_stats_for_period(usernames, orgs, week_start, week_end, use_cache, refresh)
        weekly_data[week_label] = stats

        # Show totals for this week
        total = sum(s.total() for s in stats.values())
        suffix = " (cached)" if will_use_cache else ""
        print(f"({total} activities){suffix}")

    return weekly_data


def get_metric_value(stats: UserStats, metric: str) -> int:
    """Get the value for a specific metric from UserStats.

    Raises:
        ValueError: If metric is not a valid metric name.
    """
    metric_accessors = {
        "prs-authored": lambda s: s.prs_authored,
        "prs-reviewed": lambda s: s.prs_reviewed,
        "prs-commented": lambda s: s.prs_commented,
        "issues-authored": lambda s: s.issues_authored,
        "issues-commented": lambda s: s.issues_commented,
        "total": lambda s: s.total(),
    }

    if metric not in metric_accessors:
        raise ValueError(f"Unknown metric: {metric}. Valid metrics: {', '.join(VALID_METRICS)}")

    return metric_accessors[metric](stats)


def render_trend_charts(
    weekly_data: dict[str, dict[str, UserStats]],
    usernames: list[str],
    metrics: list[str],
    by_user: bool = False
) -> None:
    """Render ASCII line charts using plotext.

    Args:
        weekly_data: Weekly statistics by user
        usernames: List of usernames
        metrics: List of metrics to chart
        by_user: If True, show separate lines per user; if False, show totals only
    """
    if not PLOTEXT_AVAILABLE:
        print("Error: plotext is required for trend charts. Install with: pip install plotext")
        sys.exit(1)

    weeks = list(weekly_data.keys())

    metric_titles = {
        "prs-authored": "PRs Authored",
        "prs-reviewed": "PRs Reviewed",
        "prs-commented": "PRs Commented",
        "issues-authored": "Issues Authored",
        "issues-commented": "Issues Commented",
        "total": "Total Engagements",
    }

    # Use numeric x-axis with week labels as xticks
    x_values = list(range(len(weeks)))

    for metric in metrics:
        if metric not in VALID_METRICS:
            continue

        title = metric_titles.get(metric, metric)

        plt.clear_figure()
        plt.plotsize(80, 15)
        plt.theme("dark")
        plt.canvas_color("default")  # Use terminal background (transparent)
        plt.axes_color("default")    # Make axes area transparent too
        plt.title(title)

        # Calculate values and track max for y-axis scaling
        all_values = []
        if by_user and len(usernames) > 1:
            # Show separate lines per user
            for username in usernames:
                values = [get_metric_value(weekly_data[week].get(username, UserStats()), metric) for week in weeks]
                all_values.extend(values)
                plt.plot(x_values, values, label=username, marker="braille")
        else:
            # Show totals (aggregate all users)
            values = [
                sum(get_metric_value(weekly_data[week].get(u, UserStats()), metric) for u in usernames)
                for week in weeks
            ]
            all_values = values
            plt.plot(x_values, values, marker="braille")

        # Set integer y-ticks with 3-digit minimum width
        max_val = max(all_values) if all_values else 0
        if max_val <= 5:
            ytick_values = list(range(0, max_val + 2))
        else:
            step = max(1, (max_val + 4) // 5)
            ytick_values = list(range(0, max_val + step + 1, step))
        ytick_labels = [f"{v:3d}" for v in ytick_values]
        plt.yticks(ytick_values, ytick_labels)

        plt.xticks(x_values, weeks)
        plt.xlabel("Week")
        plt.ylabel("Count")

        plt.show()
        print()  # Add spacing between charts


def parse_arguments() -> argparse.Namespace:
    """Parse and validate command-line arguments."""
    parser = argparse.ArgumentParser(
        description="Fetch GitHub issues and PRs a user has interacted with in an organization."
    )
    parser.add_argument(
        "--username", "-u",
        required=True,
        help="GitHub username(s) to search for (comma-separated)"
    )
    parser.add_argument(
        "--org", "-o",
        required=True,
        help="GitHub organization(s) to search in (comma-separated)"
    )
    parser.add_argument(
        "--days", "-d",
        type=int,
        default=7,
        help="Number of days to look back (default: 7)"
    )
    parser.add_argument(
        "--end-date", "-e",
        type=parse_date_arg,
        default=None,
        help="End date for the search window (format: YYYY-MM-DD, default: today)"
    )
    parser.add_argument(
        "--last-week",
        action="store_true",
        help="Search the last complete week (Monday to Sunday)"
    )
    parser.add_argument(
        "--show-filtered",
        action="store_true",
        help="Show filtered out items in a separate section"
    )
    parser.add_argument(
        "--trend", "-t",
        action="store_true",
        help="Enable trend mode: analyze multiple weeks and display charts"
    )
    parser.add_argument(
        "--weeks", "-w",
        type=int,
        default=4,
        help="Number of weeks to analyze in trend mode (default: 4)"
    )
    parser.add_argument(
        "--metrics", "-m",
        type=str,
        default=None,
        help="Comma-separated metrics to chart: prs-authored,prs-reviewed,prs-commented,issues-authored,issues-commented,total (default: all)"
    )
    parser.add_argument(
        "--by-user",
        action="store_true",
        help="In trend mode, show separate chart lines per user instead of totals"
    )
    parser.add_argument(
        "--refresh",
        action="store_true",
        help="Force refresh: fetch fresh data and update cache"
    )
    parser.add_argument(
        "--no-cache",
        action="store_true",
        help="Skip cache entirely: don't read from or write to cache"
    )
    parser.add_argument(
        "--debug",
        action="store_true",
        help="Show verbose output (progress messages, per-repo details)"
    )
    parser.add_argument(
        "--clear-cache",
        action="store_true",
        help="Clear all cached data and exit"
    )
    return parser.parse_args()


def calculate_date_range(args: argparse.Namespace) -> tuple[datetime, datetime]:
    """Calculate the date range based on arguments."""
    if args.last_week:
        return get_last_complete_week()
    elif args.end_date:
        end_dt = args.end_date.replace(hour=23, minute=59, second=59)
        since_dt = end_dt - timedelta(days=args.days)
        return since_dt, end_dt
    else:
        end_dt = datetime.now(timezone.utc)
        since_dt = end_dt - timedelta(days=args.days)
        return since_dt, end_dt


def parse_user_inputs(args: argparse.Namespace) -> tuple[list[str], list[str]]:
    """Parse and validate usernames and organizations."""
    usernames = [u.strip() for u in args.username.split(",") if u.strip()]
    orgs = [o.strip() for o in args.org.split(",") if o.strip()]

    if not usernames:
        print("Error: No valid usernames provided", file=sys.stderr)
        sys.exit(1)
    if not orgs:
        print("Error: No valid organizations provided", file=sys.stderr)
        sys.exit(1)

    return usernames, orgs


def run_trend_mode(
    args: argparse.Namespace,
    usernames: list[str],
    orgs: list[str]
) -> None:
    """Handle trend mode execution."""
    if not PLOTEXT_AVAILABLE:
        print("Error: plotext is required for trend charts. Install with: pip install plotext")
        sys.exit(1)

    # Parse and validate metrics
    if args.metrics:
        metrics = [m.strip() for m in args.metrics.split(",")]
        invalid = set(metrics) - VALID_METRICS
        if invalid:
            print(f"Error: Invalid metrics: {', '.join(invalid)}")
            print(f"Valid metrics: {', '.join(ALL_METRICS)}")
            sys.exit(1)
    else:
        metrics = ALL_METRICS

    use_cache = not args.no_cache
    cache_status = ""
    if args.no_cache:
        cache_status = " (cache disabled)"
    elif args.refresh:
        cache_status = " (refreshing cache)"

    users_str = ", ".join(f"@{u}" for u in usernames)
    orgs_str = ", ".join(orgs)
    print(f"Fetching {args.weeks}-week trend for {users_str} in {orgs_str}{cache_status}...\n")

    weekly_data = collect_weekly_stats(usernames, orgs, args.weeks, use_cache, args.refresh)

    print(f"\n## Trend Charts\n")
    render_trend_charts(weekly_data, usernames, metrics, by_user=args.by_user)

    # Print summary table
    print("## Weekly Summary\n")
    weeks = list(weekly_data.keys())
    headers = ["Week"] + [m.replace("-", " ").title() for m in metrics]

    # Build all rows first to calculate column widths
    rows = []
    for week in weeks:
        values = [week]
        for metric in metrics:
            total = sum(
                get_metric_value(weekly_data[week].get(username, UserStats()), metric)
                for username in usernames
            )
            values.append(str(total))
        rows.append(values)

    # Calculate column widths
    col_widths = [len(h) for h in headers]
    for row in rows:
        col_widths = [max(cw, len(val)) for cw, val in zip(col_widths, row)]

    # Print header row
    header_row = "| " + " | ".join(h.ljust(col_widths[i]) for i, h in enumerate(headers)) + " |"
    print(header_row)

    # Print separator row
    separator = "|" + "|".join("-" * (w + 2) for w in col_widths) + "|"
    print(separator)

    # Print data rows (first column left-aligned, rest right-aligned for numbers)
    for row in rows:
        formatted_values = [row[0].ljust(col_widths[0])]  # Week label left-aligned
        formatted_values += [v.rjust(w) for v, w in zip(row[1:], col_widths[1:])]  # Numbers right-aligned
        formatted_row = "| " + " | ".join(formatted_values) + " |"
        print(formatted_row)


def print_detailed_results(
    active_items: list[dict],
    filtered_items: list[dict],
    usernames: list[str]
) -> None:
    """Print detailed per-repository results."""
    if len(usernames) == 1:
        # Single user: group by repository
        by_repo = group_by_repository(active_items)

        print(f"\n## Active Participation\n")
        print(f"Found {len(active_items)} issues/PRs with active participation across {len(by_repo)} repositories")
        if filtered_items:
            print(f"(filtered out {len(filtered_items)} items with only passive/old interactions)\n")

        for repo_name in sorted(by_repo.keys()):
            repo_items = by_repo[repo_name]
            print(f"\n### {repo_name} ({len(repo_items)} items)\n")
            print("| Type | State | Title | Interactions |")
            print("|------|-------|-------|--------------|")
            for item in repo_items:
                print(format_table_row(item))
    else:
        # Multiple users: group by user, then by repository
        print(f"\n## Active Participation\n")
        print(f"Found {len(active_items)} unique issues/PRs with active participation")
        if filtered_items:
            print(f"(filtered out {len(filtered_items)} items with only passive/old interactions)\n")

        for username in usernames:
            # Filter items for this user with active interactions
            user_items = []
            for item in active_items:
                user_interactions = [
                    i for i in item.get("_interactions", [])
                    if f"(@{username})" in i
                ]
                if not user_interactions:
                    continue

                # Check if user has active (non-passive) interactions
                base_interactions = {parse_interaction(i)[0] for i in user_interactions}
                if not base_interactions or base_interactions.issubset(PASSIVE_INTERACTION_TYPES):
                    continue

                item_copy = item.copy()
                item_copy["_interactions"] = user_interactions
                user_items.append(item_copy)

            if not user_items:
                continue

            # Group by repository
            by_repo = group_by_repository(user_items)

            print(f"\n## @{username}\n")
            print(f"Found {len(user_items)} issues/PRs across {len(by_repo)} repositories\n")

            for repo_name in sorted(by_repo.keys()):
                repo_items = by_repo[repo_name]
                print(f"\n### {repo_name} ({len(repo_items)} items)\n")
                print("| Type | State | Title | Interactions |")
                print("|------|-------|-------|--------------|")
                for item in repo_items:
                    print(format_table_row(item))


def print_summary_table(user_stats: dict[str, UserStats]) -> None:
    """Print the summary statistics table."""
    print(f"\n## Summary\n")

    # Define column headers
    headers = ["Username", "PRs Authored", "PRs Reviewed", "PRs Commented", "Issues Authored", "Issues Commented", "Total"]

    # Sort by total activity (sum of all interactions) descending
    sorted_users = sorted(
        user_stats.items(),
        key=lambda x: x[1].total(),
        reverse=True
    )

    # Calculate column widths
    col_widths = [len(h) for h in headers]
    for username, stats in sorted_users:
        values = [username] + [str(v) for v in stats.as_list()] + [str(stats.total())]
        col_widths = [max(cw, len(val)) for cw, val in zip(col_widths, values)]

    # Print header row
    header_row = "| " + " | ".join(h.ljust(col_widths[i]) for i, h in enumerate(headers)) + " |"
    print(header_row)

    # Print separator row
    separator = "|" + "|".join("-" * (w + 2) for w in col_widths) + "|"
    print(separator)

    # Print data rows (username left-aligned, numbers right-aligned)
    for username, stats in sorted_users:
        values = [username] + [str(v) for v in stats.as_list()] + [str(stats.total())]
        formatted_values = [values[0].ljust(col_widths[0])]  # Username left-aligned
        formatted_values += [v.rjust(w) for v, w in zip(values[1:], col_widths[1:])]  # Numbers right-aligned
        row = "| " + " | ".join(formatted_values) + " |"
        print(row)

    # Print totals row
    if sorted_users:
        # Calculate totals for each column
        totals = [0] * 5  # 5 stat columns
        for _, stats in sorted_users:
            stats_list = stats.as_list()
            totals = [t + s for t, s in zip(totals, stats_list)]

        # Print separator before totals
        print(separator)

        # Print totals row, including a grand total (sum of all totals)
        grand_total = sum(totals)
        total_values = ["**Total**"] + [str(v) for v in totals] + [str(grand_total)]
        formatted_totals = [total_values[0].ljust(col_widths[0])]  # Label left-aligned
        formatted_totals += [v.rjust(w) for v, w in zip(total_values[1:], col_widths[1:])]  # Numbers right-aligned
        total_row = "| " + " | ".join(formatted_totals) + " |"
        print(total_row)


def print_filtered_items(filtered_items: list[dict]) -> None:
    """Print the filtered items section."""
    print(f"\n## Filtered Out ({len(filtered_items)} items)\n")
    print("Items with only passive interactions (review-requested, assignee, mentioned) or no recent activity:\n")

    # Group filtered by repository
    filtered_by_repo = group_by_repository(filtered_items)

    for repo_name in sorted(filtered_by_repo.keys()):
        repo_items = filtered_by_repo[repo_name]
        print(f"\n### {repo_name} ({len(repo_items)} items)\n")
        print("| Type | State | Title | Interactions |")
        print("|------|-------|-------|--------------|")
        for item in repo_items:
            print(format_table_row(item))


def main() -> None:
    """Main entry point for the script."""
    args = parse_arguments()

    # Handle --clear-cache early (doesn't require gh CLI)
    if args.clear_cache:
        count = clear_cache()
        print(f"Cleared {count} cached file(s) from {get_cache_dir()}")
        return

    # Validate gh CLI is installed and authenticated
    validate_gh_cli()

    # Parse inputs and calculate date range
    since_dt, end_dt = calculate_date_range(args)
    usernames, orgs = parse_user_inputs(args)

    # Handle trend mode
    if args.trend:
        run_trend_mode(args, usernames, orgs)
        return

    # Fetch activity using shared function
    use_cache = not args.no_cache
    result = fetch_activity(
        usernames, orgs, since_dt, end_dt,
        use_cache=use_cache,
        refresh=args.refresh,
        cache_full_items=True,
        debug=args.debug
    )

    if not result.active_items and not result.filtered_items:
        print("No activity found.")
        return

    # Print detailed results (only in debug mode)
    if args.debug:
        print_detailed_results(result.active_items, result.filtered_items, usernames)

    # Print summary table
    print_summary_table(result.user_stats)

    # Show filtered items if requested
    if args.show_filtered and result.filtered_items:
        print_filtered_items(result.filtered_items)


if __name__ == "__main__":
    main()
